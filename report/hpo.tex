\documentclass[twoside,11pt]{article}
\usepackage[T1]{fontenc}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{bbold}
\usepackage{thm-restate}
%\usepackage{bbold}
\usepackage[ruled,linesnumbered,lined,commentsnumbered]{algorithm2e}
\usepackage[colorinlistoftodos, textwidth=22mm, shadow]{todonotes}
\definecolor{blued}{RGB}{70,197,221}
\definecolor{pearOne}{HTML}{2C3E50}
%A9CF54
\definecolor{pearTwo}{HTML}{A9CF54}
\definecolor{pearTwoT}{HTML}{C2895B}
\definecolor{pearThree}{HTML}{FF69B4}
\colorlet{titleTh}{pearOne}
\colorlet{bull}{pearTwo}
\definecolor{pearcomp}{HTML}{B97E29}
\definecolor{pearFour}{HTML}{588F27}
\definecolor{pearFith}{HTML}{ECF0F1}
\definecolor{pearDark}{HTML}{2980B9}
\definecolor{pearDarker}{HTML}{F330DB}
\hypersetup{
	colorlinks,
	citecolor=pearDark,
	linkcolor=pearThree,
	urlcolor=pearDarker}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

%\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\input{macros}

\newtheorem{assumption}{Assumption}
\firstpageno{1}

\begin{document}

\title{\large Hyper-parameter optimization}

\author{\name Xuedong Shang \email xuedong.shang@inria.fr}

\editor{}

\maketitle


\begin{abstract}

\end{abstract}


\section{Introduction}

Modern machine learning algorithms often contain many nuisance parameters that cannot be learned through the learning process, but instead, need to be manually specified. It is thus appealing to design algorithms that require fewer such so-called \emph{hyper-parameters} (not always feasible though).

An alternative is to perform the \emph{hyper-parameter optimization} (HPO) that can be viewed as a \emph{black-box/global optimization} problem where function evaluations are supposed to be very expensive. Here a typical function evaluation involves running the primary machine learning algorithm to completion on a large and high-dimensional dataset, which often takes a considerable amount of time or resources. This limits vastly the number of evaluations that could be carried out, which makes it desirable to design efficient high-level algorithms that automate this tuning procedure.

\paragraph{Related literature}

Several na√Øve but traditional ways of performing the hyper-parameter search exist, such as \Grid\ and \Random. 

\Grid\ is an old-fashioned but commonly used method in a lot of research papers before 2010s~\citep{lecun1998gradient}, as well as in many machine learning softwares such as \LIBSVM~\citep{chang2011libsvm} and \Scikit~\citep{pedregosa2011sklearn}. It simply carries out an \emph{exhaustive} searching of parameters through a manually specified \emph{finite} subset of the hyper-parameter search space. \Grid\ clearly suffers from the \emph{curse of dimensionality}, which makes it undesirable in real applications.

\Random\ overcomes this problem by randomly picking hyper-paramter configurations from the search space, and can be easily generalized to continuous space as well. It is shown to outperform \Grid\ especially in the case that the \emph{intrinsic dimensionality} of the optimization problem is low~\citep{bergstra2012random}. Hence we use \Random\ as a baseline method here in this section. Note that both methods are \emph{embarrasingly parallel}, which can be a very strong point in some situations.

Recent trending solutions for HPO are mostly Bayesian-based~\citep{bergstra2011tpe,hutter2011smac,snoek2012spearmint,snoek2015}. \emph{Bayesian optimization} (BO) depends on a prior belief, typically a Gaussian process (GP), on the target function that we can update to a posterior distribution over the function space where lies the target function given a sequence of observations. It then decides where to sample next with the help of an \emph{utility/acquisition function} by maximizing/minimizing the utilitly w.r.t.\ the posterior distribution. Typical acquisition functions include \EI, \PI~\citep{mockus1978} and \GPUCB~\citep{srinivas2010gpucb} (see~\citealt{brochu2010bayesian,shahriari2016loop} for a survey). Note that classic BO methods with GP prior and typical acquisition functions can be applied straightforwardly to hyper-parameter tuning (\citealt{snoek2012spearmint} provides a Python package called \Spearmint\ to perform the task), some variants like \TPE~\citep{bergstra2011tpe} and \SMAC~\citep{hutter2011smac} are more commonly used though.

\begin{remark}
	One may notice that a new optimization task emerges in a Bayesian optimization procedure, that is to optimize the acquisition function. It may seem to be a bit artificial, but this acquisition function is usually much more regular compared to the target function, thus easier to optimize.
\end{remark}

Bayesian optimization focuses on adaptively choosing different parameter configurations based on previous observations, but always run the primary machine learning classifier/regressor into completion given a set of hyper-parameters. In a bandit point of view, this setting can be considered as a \emph{stochastic infinitely-armed bandit} (SIAB) problem.

Another take on this problem is to adaptively allocate \emph{resources} to more promising configurations. Resources here can be time, dataset subsampling, feature subsampling, etc. In such a setting, the classifier is not always trained into completion given a parameter configuration, but rather is stopped early if it is shown to be bad so that we can allocate more resources to other configurations. This idea of early stopping is proposed by~\cite{li2016}, which states the HPO problem as a \emph{non-stochastic infinitely-armed bandit} (NIAB) problem.

In this report, we treat both the two settings mentioned previously. Before that, we need to mention that totally different types of approaches also exist, for example \emph{evolutionary optimization} (EO). EO follows a process inspired by the biological concept of \emph{evolution}, which repeatedly replaces the worst-performing hyper-parameter configurations from a randomly initialized population of solutions.

Finally, hierarchical bandit algorithms like \HOO~\citep{bubeck2010x}, \HCT~\citep{azar2014online} could be potential candidates for HPO as well. And to the best of our knowledge, these methods have never been investigated in the HPO literature.

\section{\Hyperband coupled with a \TS-like algorithm}

We propose a new heuristic based on the Top-Two Thompson Sampling (\TTTS, see~\citealt{russo2016ttts}. The idea is similar to \Hyperband. By running several brackets of \TTTS with different number of configurations $n$ and the same budget, we try to trade off between the number of configurations and the number of resources allocated to each configuration. Unlike \Hyperband where the number of resources is fixed for each configuration in every bracket, here the number of resources is decided by the underlying \TS.

This heuristic requires three inputs $\beta$, $\gamma$, $B$, and $s_{\operatorname{max}}$:  (1) $\gamma$ characterizes how fast does the number of configurations we want to evaluate in each bracket shrink, (2) $B$ is the total budget, and (3) $s_{\operatorname{max}}$ represents the index of the largest bracket (containing the largest number of configurations).

The subroutine used here is \TTTS where we make use of a prior distribution $\Pi_1$ over a set of parameters $\Theta$, where for each $\mathbf{\theta}\in\Theta$, $\theta_i$ caraterizes configuration $i$ for $i\in\{1\ldots n\}$. Based on a sequence of observations, we can update our beliefs to attain a posterior distribution $\Pi_t$. At each time step $t$, the subroutine samples $\theta$ from $\Pi_t$, then with probability $\beta$, the subroutine evaluates the configuration $I$ with the best paramter $\theta_I = \max_{c\in C}\hat{\theta}_c$. Otherwise it samples a new $\theta$ from $\Pi_t$ until we obtain a different configuration $J\neq I$ such that $\theta_J = \max_{c\in C}\hat{\theta}_c$.

\begin{remark}
Note that this algorithm does not require computing or approximating the optimal action probabilities, which could be computationally heavy. However in practice, sometimes it could be ridiculously long to sample a $J$ that is different from $I$, especially when the best configuration is far better than the others. To avoid this issue, we can either explicitly compute the optimal action probabilities, or just play the second best one when this kind of situation occurs.
\end{remark}

\begin{algorithm}[h]
\SetKwInOut{Input}{Input}
\SetKwInOut{Init}{Initialize}

\Input{$\beta$; $\gamma$; $B$; $s_{\operatorname{max}}$}
\Init{$\texttt{budget}=\floor{B/s_{\operatorname{max}}}$; $L=\emptyset$; $t = 0$}
\For{$s \leftarrow s_{\operatorname{max}}$ \KwTo $0$}{
	$n = \ceil{\frac{s_{\operatorname{max}}+1}{s+1}\gamma^s}$\; 
	$C = \operatorname{get\_hyperparameter\_configurations(n)}$\;
    	\For{$c\in C$}{
		$L = L\cup \{\operatorname{run\_then\_return\_val\_loss}(c)\}$\;}
	$t = n$\;
	// Begin \TTTS\;
	\While{$t < \texttt{budget}$}{
		Sample $\mathbf{\hat{\theta}} \sim \Pi_t$; $I \leftarrow \argmax_{c\in C}\hat{\theta}_c$\;
		Sample $b \sim \operatorname{Bernoulli}(\beta)$\;
		\eIf{$b = 1$}{
    		$L = L\cup \{\operatorname{run\_then\_return\_val\_loss}(I)\}$\;}{
    		Repeat sample $\mathbf{\hat{\theta}} \sim \Pi_t$; $J \leftarrow \argmax_{c\in C}\hat{\theta}_c$ until $I\neq J$\;
		$L = L\cup \{\operatorname{run\_then\_return\_val\_loss}(J)\}$\;}
		Update $\Pi_t$\;
		$t = t+1$\;}
	}
\Return{Configuration\ with\ the\ smallest\ intermediate\ loss\ seen\ so\ far}
\caption{Heuristic (based on \TTTS)\label{heuristic1}}
\end{algorithm}

The pseudo-code is shown in Algorithm~\ref{heuristic1}. Let us give some more details on how to update the posterior belief (Line. 17 in Algorithm~\ref{heuristic1}). Currently, we assume a Beta prior which is usually associated with Bernoulli bandits. Here in our case, however, the reward (or more precisely, the loss) lies in $[0, 1]$, we thus need to adapt the Thompson Sampling process to the general stochastic bandits case. One way to tackle this is the binarization trick shown in Algorithm~\ref{annexe1} inspired by~\cite{agrawal2012}. 

\begin{algorithm}[h]
\SetKwInOut{Input}{Input}
\SetKwInOut{Init}{Initialize}

\Input{$n \leftarrow |C| (\operatorname{number\ of\ arms})$, $\alpha_0$, $\beta_0$, $\beta$}
\Init{$\forall c\in C, S_c=0 (\operatorname{number\ of\ successes}), F_c=0 (\operatorname{number\ of\ failures})$}
\For{$t \leftarrow 1$ \KwTo $B$}{
	$\forall c\in C$, sample $\hat{\theta}_c \sim \operatorname{Beta}(S_c+\alpha_0, F_c+\beta_0)$; $I \leftarrow \argmax_{c\in C}\hat{\theta}_c$\;
	Sample $b \sim \operatorname{Bernoulli}(\beta)$\;
	\eIf{$b = 1$}{
    		Evaluate configuration $I$, and observe loss $\tilde{l}_t$\;}{
    		Repeat $\forall c\in C$, sample $\hat{\theta}_c \sim \operatorname{Beta}(S_c+\alpha_0, F_c+\beta_0)$; $J \leftarrow \argmax_{c\in C}\hat{\theta}_c$ until $I\neq J$\;
		Set $I \leftarrow J$\;
		Evaluate configuration $I$, and observe loss $\tilde{l}_t$\;}
	Sample $r_t \sim \operatorname{Bernoulli}(1-\tilde{l}_t)$\;
	\eIf{$r_t = 1$}{
		$S_I \leftarrow S_I + 1$\;}{
		$F_I \leftarrow F_I + 1$\;}
	$t = t+1$\;
	}
\caption{Detailed \TTTS with Beta prior for general stochastic bandits\label{annexe1}}
\end{algorithm}

\section{Hyper-parameter tuning as NIAB problem}

We consider in this section the non-stochastic setting where several classifiers such as logistic regression, Multi-Layer Perceptron (\MLP) and Convolutional Neural Networks (\CNN) are trained on the \MNIST dataset using mini-batch \emph{stochastic gradient descent} (\SGD). This part of code (code for classifers with eventual usage of GPU) is based on code available at \url{http://deeplearning.net/}. 

\paragraph{\textbf{Dataset}} The \MNIST dataset is pre-split into three parts: training set $D_{\operatorname{train}}$, validation set $D_{\operatorname{valid}}$ and test set $D_{\operatorname{test}}$.

\paragraph{\textbf{Hyper-parameters}} The hyper-parameters to be optimized are listed below in Table \ref{logistic_sgd}, \ref{mlp_sgd} and \ref{cnn_sgd}. For logistic regression, the hyper-parameters to be considered are learning rate and mini-batch size (since we are doing mini-batch \SGD). For \MLP, we take into account an additional hyper-parameter which is the $l_2$ regularization factor. For \CNN, we take into account the number of kernels used in the two convolutional-pooling layers.

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Hyper-parameter} & \textbf{Type}                      & \textbf{Bounds}               \\ \midrule
$\operatorname{learning\_rate}$                & $\mathbb{R}^+$ & $\left[ 10^{-3}, 10^{-1} \right]$ (log-scaled) \\
$\operatorname{batch\_size}$           & $\mathbb{N}^+$ & $\left[1, 1000 \right]$         \\ \bottomrule
\end{tabular}
\caption{Hyper-parameters to be optimized for logistic regression with \SGD.}
\label{logistic_sgd}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Hyper-parameter} & \textbf{Type}                      & \textbf{Bounds}               \\ \midrule
$\operatorname{learning\_rate}$                & $\mathbb{R}^+$ & $\left[ 10^{-3}, 10^{-1} \right]$ (log-scaled) \\
$\operatorname{batch\_size}$           & $\mathbb{N}^+$ & $\left[1, 1000 \right]$         \\ 
$\operatorname{l_2\_reg}$		& $\mathbb{R}^+$ & $\left[ 10^{-4}, 10^{-2} \right]$ (log-scaled) \\ \bottomrule
\end{tabular}
\caption{Hyper-parameters to be optimized for \MLP with \SGD.}
\label{mlp_sgd}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Hyper-parameter} & \textbf{Type}                      & \textbf{Bounds}               \\ \midrule
$\operatorname{learning\_rate}$                & $\mathbb{R}^+$ & $\left[ 10^{-3}, 10^{-1} \right]$ (log-scaled) \\
$\operatorname{batch\_size}$           & $\mathbb{N}^+$ & $\left[1, 1000 \right]$         \\
$\operatorname{k_2}$           & $\mathbb{N}^+$ & $\left[10,  60 \right]$         \\
$\operatorname{k_1}$           & $\mathbb{N}^+$ & $\left[5,  k_2 \right]$         \\ \bottomrule
\end{tabular}
\caption{Hyper-parameters to be optimized for \CNN with \SGD.}
\label{cnn_sgd}
\end{table}

\paragraph{\textbf{Resource Allocation}} The type of resource considered here is the number of epochs, where one epoch means a pass of training through the whole training set using \SGD. Note that this is similar to the original Hyperband paper where one unit of resources corresponds to 100 mini-batch iterations for example. One epoch may contain a various number of mini-batch iterations depending on the mini-batch size.

\paragraph{\textbf{Experimental Design}} In this section, we focus on neural network-typed classifiers, we will thus be maximizing the likelihood of the training set $D_{\operatorname{train}}$ under the model parameterized by $\theta$ (in this section, $\theta$ corresponds to $(W, b)$ where $W$ is the weight matrix and $b$ is the bias vector):
\[
\mathcal{L}(\theta, D_{\operatorname{train}}) = \frac{1}{|D_{\operatorname{train}}|} \sum_{i=1}^{|D_{\operatorname{train}}|} \log (\mathbb{P}(Y=y^{(i)}|x^{(i)},\theta),
\]
which is equivalent to minimize the loss function:
\[
\ell(\theta, D_{\operatorname{train}}) = -\mathcal{L}(\theta, D_{\operatorname{train}}).
\]

At each time step $t$, we give one unit of resources (one epoch of training here) to the current algorithm, who will run one epoch of training on the training set $D_{\operatorname{train}}$. The trained model is then used to predict output values $\hat{y}_{\operatorname{pred},t}$ and $\tilde{y}_{\operatorname{pred},t}$ respectively over validation set $D_{\operatorname{valid}}$ and test set $D_{\operatorname{test}}$. We then compute the number that were misclassified by the model, a.k.a. the zero-one loss on the validation and test set:
\[
\ell_t(\hat{\theta}_t, D_{\operatorname{valid}}) = \frac{1}{|D_{\operatorname{valid}}|} \sum_{i=1}^{|D_{\operatorname{valid}}|} \mathbb{1}_{\{\hat{y}_{\operatorname{pred},t}^{(i)} \neq y^{(i)}\}},
\]
\[
\ell_t(\hat{\theta}_t, D_{\operatorname{test}}) = \frac{1}{|D_{\operatorname{test}}|} \sum_{i=1}^{|D_{\operatorname{test}}|} \mathbb{1}_{\{\tilde{y}_{\operatorname{pred},t}^{(i)} \neq y^{(i)}\}}.
\]

During the experiment, we keep track of the best validation error and its associated test error. At each time step $t$, if the new validation error is smaller than the current best validation error, then we update the best validation error, and report the new test error. Otherwise we just report the test error associated with the previous best validation error.

Note that there is a very important 'keep training' notion here, which means if we are running the classifier with a same hyper-parameter configuration, then we do not restart the training from scratch. In contrary, we always keep track of previously trained weight matrix and bias vector with respect to the current hyper-parameter configuration, and train the model from these pre-trained parameters. 

The total budget for Hyperband and the heuristic would be $B = R\times s_{\max}$, and each configuration can be evaluted for $R_{\max}$ times (this $R_{\max}$ depends only on $R$ and $s_{\max}$). For HCT, we just need to feed the algorithm the total budget $B$, and the number of times that each configuration is evaluated will be decided by the algorithm itself. While for TPE, HOO and Random Search, we  evaluate each configuration for $R_{\max}$ times in order to make a fair comparison, which means $B/R_{\max}$ configurations will be evaluted.

\paragraph{\textbf{Comparison}}
All plots here (from Fig.~\ref{logistic_1} to Fig.~\ref{mlp_0}) are averaged on 10 trials of experiments.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/mnist/logistic_1.pdf}
    \caption{Comparing different hyper-parameter optimization algorithms on Logistic Regression, trained on \MNIST Dataset.}
    \label{logistic_1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/mnist/logistic_1_bis.pdf}
    \caption{Comparing \Hyperband and the heuristic on Logistic Regression, trained on \MNIST Dataset.}
    \label{logistic_1_bis}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/mnist/mlp_0_bis.pdf}
    \caption{Comparing different hyper-parameter optimization algorithms on \MLP, trained on \MNIST Dataset.}
    \label{mlp_0}
\end{figure}

\paragraph{\textbf{Discussion}} In the current setting, Hyperband seems to be a plausible choice since it may explore more configurations compared to other algorithms. And comparing Hyperband and the heuristic, we can see that they almost performs as well as each other. There are two major observations here. First, the heuristic seems to be converging more smoothly than Hyperband, this is because Hyperband focuses on evaluating one of the configurations after its round-robin tour among all the configurations in the bracket, which leads to a straight drop of the output loss. The second observation is that the heuristic seems to have a slightly better output at the end. This could be due to the fact that\TTTS is likely to evaluate the possible best configurations more times than Sequential Halving.


\section{Hyper-parameter tuning as SIAB problem}

We consider here Adaptive Boosting (AdaBoost), Gradient Boosting Machine (GBM), k-Nearest Neighbors (KNN), Multi-Layer Perceptron (\MLP), Support Vector Machine (SVM), Decision Tree and Random Forest from Scikit-learn.

\paragraph{\textbf{Dataset}} Several datasets on UCI dataset archive (e.g. Wine, Breast Cancer, etc) are being used. They are all pre-split into a training set $D_{\operatorname{train}}$ and a test set $D_{\operatorname{test}}$.

\paragraph{\textbf{Hyper-parameters}} The hyper-parameters to be optimized are listed below in Table \ref{adaparam}, \ref{gbmparam},  \ref{knnparam}, \ref{mlpparam}, \ref{svmparam}, \ref{treeparam} and \ref{rfparam}.

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter}             & \textbf{Type}  & \textbf{Bounds}                          \\ \midrule
\texttt{learning\_rate}      & $\mathbb{R}^+$ & $\left[10^{-5}, 10^{-1}\right]$                         \\
\texttt{n\_estimators}       & Integer        & $\left\lbrace 5,\dots, 200 \right\rbrace$
\end{tabular}
\caption{Hyper-parameters to be optimized for AdaBoost models.}
\label{adaparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter}             & \textbf{Type}  & \textbf{Bounds}                          \\ \midrule
\texttt{learning\_rate}      & $\mathbb{R}^+$ & $\left[10^{-5}, 10^{-2}\right]$                         \\
\texttt{n\_estimators}       & Integer        & $\left\lbrace 10,\dots, 100 \right\rbrace$ \\
\texttt{max\_depth}          & Integer        & $\left\lbrace 2, \dots, 100 \right\rbrace$ \\
\texttt{min\_samples\_split}  & Integer        & $\left\lbrace 2, \dots, 100 \right\rbrace$
\end{tabular}
\caption{Hyper-parameters to be optimized for GBM models.}
\label{gbmparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Bounds}                           \\ \midrule
$k$                & Integer       & $\left\lbrace 10, \dots,50 \right\rbrace$
\end{tabular}
\caption{Hyper-parameters to be optimized for KNN models.}
\label{knnparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{lll}
\hline
\textbf{Parameter}             & \textbf{Type}    & \textbf{Bounds}       \\ \hline
\texttt{hidden\_layer\_size} & Integer          & $\left[5, 50\right]$  \\
\texttt{alpha}               & $\mathbb{R}^{+}$ & $\left[0, 0.9\right]$
\end{tabular}
\caption{Hyper-parameters to be optimized for \MLP models.}
\label{mlpparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter} & \textbf{Type}                      & \textbf{Bounds}               \\ \midrule
$C$                & $\mathbb{R}^+$ & $\left[ 10^{-5}, 10^{5} \right]$ (log-scaled) \\
$\gamma$           & $\mathbb{R}^+$ & $\left[10^{-5}, 10^{5} \right]$  (log-scaled)       \\ \bottomrule
\end{tabular}
\caption{Hyper-parameters to be optimized for SVM models.}
\label{svmparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter}             & \textbf{Type}  & \textbf{Bounds}                          \\ \midrule
\texttt{max\_features}      & $\mathbb{R}^+$ & $\left[0.01, 0.99\right]$   \\
\texttt{max\_depth}          & Integer        & $\left\lbrace 4, \dots, 30 \right\rbrace$ \\
\texttt{min\_samples\_split}  & $\mathbb{R}^+$  & $\left[0.01, 0.99\right]$
\end{tabular}
\caption{Hyper-parameters to be optimized for Decision Tree models.}
\label{treeparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter}             & \textbf{Type}  & \textbf{Bounds}                          \\ \midrule
\texttt{max\_features}      & $\mathbb{R}^+$ & $\left[0.1, 0.5\right]$   \\
\texttt{n\_estimators}          & Integer        & $\left\lbrace 10, \dots, 50 \right\rbrace$ \\
\texttt{min\_samples\_split}  & $\mathbb{R}^+$  & $\left[0.1, 0.5\right]$
\end{tabular}
\caption{Hyper-parameters to be optimized for Random Forest models.}
\label{rfparam}
\end{table}

\paragraph{\textbf{Resource Allocation}} One unit of resources in this setting is one iteration of training, which means one complete training of each classifier/regressor over the whole training set.  

\paragraph{\textbf{Experimental Design}} In this section we use the logarithmic loss, also known as cross-entropy for classification problem, defined by:
\[
\ell(\theta, D) = - \frac{1}{|D|} \sum_{i=1}^{|D|} \sum_{j=1}^m y^{(i)}_{j}\log(\hat{p}^{(i)}_{j}),
\]
where $\hat{p}_{ij}$ is the predicted probability of a sample $i$ belonging to class $j$, and $m$ is the number of classes considered. And for regression problems, the loss that we use is the typical mean squared error, defined by:
\[
\ell(\theta, D) = - \frac{1}{|D|} \sum_{i=1}^{|D|} \left(y^{(i)} - \hat{y}^{(i)}_{\operatorname{pred}}\right)^2.
\]

And for this part of experiments, we choose to perform a shuffled $k=5$ cross-validation scheme on $D_{\operatorname{train}}$ at each time step $t$. In practice, this means that we fit $5$ models with the same architecture to different train/validation splits and average the loss results in each. More precisely, for every cross-validation split 
$\mathtt{cv}_j,j=1\ldots5$, we get a loss $\ell_{j,t}(\hat{\theta}_{j,t}, D_{\operatorname{valid},j,t}) = \dfrac{1}{n} \sum^{n}_{i=1} \left(y^{(i)}_{j} - \hat{y}^{(i)}_{\operatorname{pred},j,t}\right)^2$, where $n=|D_{\operatorname{valid}}|$ (here we take MSE as an example, it's the same for log-loss). Thus the validation loss at time $t$ is
\[
\frac{1}{5}\sum_{j=1}^{5}\ell_{j,t}(\hat{\theta}_{j,t}, D_{\operatorname{valid},j,t}) = \frac{1}{5n}\sum_{j=1}^{5} \sum_{i=1}^n \left(y^{(i)}_{j} - \hat{y}^{(i)}_{\operatorname{pred},j,t}\right)^2.
\]
Just like in the previous section, we can then compute and report the test error on the holdout test set $D_{\operatorname{test}}$:
\[
\ell_{t}(\hat{\theta}_t, D_{\operatorname{test}}) = \frac{1}{|D_{\operatorname{test}}|} \sum_{i=1}^{|D_{\operatorname{test}}|} \left(y^{(i)} - \tilde{y}^{(i)}_{\operatorname{pred},t}\right)^2.
\]

Note that under this experimental environment, 'keep training' does not make sense anymore. Thus for HOO, TPE and Random Search, we only need to evaluate each configuration once, contrarily to what we did in the previous setting. While for Hyperband, we still need to evaluate each configuration for a certain times based on $R$ and $s_{\max}$.

\paragraph{\textbf{Comparison}} Each plot here is averaged on 20 runs of experiments. Fig.~\ref{ada_0} to~\ref{svm_0} display the results for Wine Dataset.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/ada_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on AdaBoost, trained on Wine Dataset.}
    \label{ada_0}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/gbm_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on GBM, trained on Wine Dataset.}
    \label{gbm_0}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/knn_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on KNN, trained on Wine Dataset.}
    \label{knn_0}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/sk_mlp_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on \MLP, trained on Wine Dataset.}
    \label{sk_mlp_0}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/svm_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on SVM, trained on Wine Dataset.}
    \label{svm_0}
\end{figure}

Fig.~\ref{ada_1} to~\ref{svm_1} display the results for Breast Cancer Dataset.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/ada_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on AdaBoost, trained on Breast Cancer Dataset.}
    \label{ada_1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/gbm_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on GBM, trained on Breast Cancer Dataset.}
    \label{gbm_1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/knn_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on KNN, trained on Breast Cancer Dataset.}
    \label{knn_1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/sk_mlp_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on \MLP, trained on Breast Cancer Dataset.}
    \label{sk_mlp_1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/svm_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on SVM, trained on Breast Cancer Dataset.}
    \label{svm_1}
\end{figure}

\paragraph{\textbf{Discussion}} In this setting, Hyperband seems to loose its advantage of exploring more configurations. It appears to me that there is no reason for HOO, TPE and Random Search to evaluate one point several times as does Hyperband.

\newpage
\vskip 0.2in
\bibliography{Major}


%\section*{Appendix}

\end{document}

