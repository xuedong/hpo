
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}
\usepackage{fullpage}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[ruled,linesnumbered,lined,commentsnumbered]{algorithm2e}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{bbold}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{expl3}
\ExplSyntaxOn
\newcommand\latinabbrev[1]{
  \peek_meaning:NTF . {% Same as \@ifnextchar
    #1\@}%
  { \peek_catcode:NTF a {% Check whether next char has same catcode as \'a, i.e., is a letter
      #1.\@ }%
    {#1.\@}}}
\ExplSyntaxOff

%Omit final dot from each def.
\def\eg{\latinabbrev{e.g}}
\def\etal{\latinabbrev{et al}}
\def\etc{\latinabbrev{etc}}
\def\ie{\latinabbrev{i.e}}
\def\aka{\latinabbrev{a.k.a}}

\newcommand{\Epsilon}{E}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Experimental Report on Hyper-parameter Optimization}

% a short form should be given in case it is too long for the running head
\titlerunning{HPO Report}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{\textbf{Xuedong Shang}}
%
\authorrunning{Xuedong Shang}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
%\institute{Universit\'e de Rennes 1, Parcours MRI \\ \email{xuedong.shang@ens-rennes.fr}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{}
\tocauthor{Xuedong Shang}
\maketitle


\begin{abstract}

\end{abstract}


\section{Introduction}

The algorithms being considered here for the moment are Hyperband~\cite{li2016}, a Bayesian-based approach TPE~\cite{bergstra2011}, two hierarchical approaches HOO~\cite{bubeck2011}, HCT~\cite{azar2014}, a baseline method random search, plus one new hyperband-liked heuristic on top of Top-Two Thompson Sampling (TTTS)~\cite{russo2016}. Current datasets used are MNIST dataset and some datasets from UCI machine learning dataset archive.

\section{Heuristics}

We propose one new heuristic based on TTTS for the moment. The idea is similar to Hyperband. By running several brackets of TTTS with different number of configurations $n$ and the same budget, we try to trade off between the number of configurations and the number of resources allocated to each configuration. Unlike Hyperband where the number of resources is fixed for each configuration in every bracket, here the number of resources is decided by the underlying Thompson Sampling.

This heuristic requires three inputs $\beta$, $\eta$ and $R$, where $\eta$ and $R$ are the same inputs as in Hyperband: $\eta$ is the discarding proportion and $R$ is the maximum amount of resources that can be allocated to a single configuration. However, in this heuristic, they do not really make sense. The reason why they are required is just to keep the same budget as for Hyperband. The subroutine used here is TTTS where we make use of a prior distribution $\Pi_1$ over a set of parameters $\Theta$, where for each $\mathbf{\theta}\in\Theta$, $\theta_i$ caraterizes configuration $i$ for $i\in\{1\ldots n\}$. Based on a sequence of observations, we can update our beliefs to attain a posterior distribution $\Pi_t$. At each time step $t$, the subroutine samples $\theta$ from $\Pi_t$, then with probability $\beta$, the subroutine evaluates the configuration $I$ with the highest posterior optimal action probability. Otherwise it samples a new $\theta$ from $\Pi_t$ until we obtain a different configuration $J$ with the highest posterior optimal action probability.

The pseudo-code is shown in Algorithm~\ref{heuristic1}.

\begin{algorithm}[h]
\SetKwInOut{Input}{Input}
\SetKwInOut{Init}{Initialize}

\Input{$\beta$; $\eta$; $R$}
\Init{$s_{\operatorname{max}} = \floor{\operatorname{log}_{\eta}(R)}$; $B=(s_{\operatorname{max}}+1)R$; $L=\emptyset$; $t = 0$}
\For{$s \leftarrow s_{\operatorname{max}}$ \KwTo $0$}{
	$n = \ceil{\frac{B}{R}\frac{\eta^s}{(s+1)}}$\; 
	$C = \operatorname{get\_hyperparameter\_configurations(n)}$\;
    	\For{$c\in C$}{
		$L = L\cup \{\operatorname{run\_then\_return\_val\_loss}(c)\}$\;}
	$t = n$\;
	\While{$t < B$}{
		Sample $\mathbf{\hat{\theta}} \sim \Pi_t$; $I \leftarrow \argmax_{c\in C}\hat{\theta}_c$\;
		Sample $b \sim \operatorname{Bernoulli}(\beta)$\;
		\eIf{$b = 1$}{
    		$L = L\cup \{\operatorname{run\_then\_return\_val\_loss}(I)\}$\;}{
    		Repeat sample $\mathbf{\hat{\theta}} \sim \Pi_t$; $J \leftarrow \argmax_{c\in C}\hat{\theta}_c$ until $I\neq J$\;
		$L = L\cup \{\operatorname{run\_then\_return\_val\_loss}(J)\}$\;}}
	}
\Return{Configuration\ with\ the\ smallest\ intermediate\ loss\ seen\ so\ far}
\caption{Heuristic (TTTS)\label{heuristic1}}
\end{algorithm}

\section{MNIST}

We consider here Logistic Regression, Multi-Layer Perceptron (MLP) and Convolutional Neural Networks (CNN) as classifiers to be hyper-optimized. This part of code (code for classifers with eventual usage of GPU) is based on code available at \url{http://deeplearning.net/}. 

\paragraph{\textbf{Dataset}} The MNIST dataset is pre-split into three parts: training set $D_{\operatorname{train}}$, validation set $D_{\operatorname{valid}}$ and test set $D_{\operatorname{test}}$.

\paragraph{\textbf{Hyper-parameters}} The hyper-parameters to be optimized are listed below in Table \ref{logistic_sgd}, \ref{mlp_sgd} and \ref{cnn_sgd}. For logistic regression, the hyper-parameters to be considered are learning rate and mini-batch size (since we are doing mini-batch SGD). For MLP, we take into account an additional hyper-parameter which is the $l_2$ regularization factor. For CNN (or LeNet), we take into account the number of kernels used in the two convolutional-pooling layers.

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Hyper-parameter} & \textbf{Type}                      & \textbf{Bounds}               \\ \midrule
$\operatorname{learning\_rate}$                & $\mathbb{R}^+$ & $\left[ 10^{-3}, 10^{-1} \right]$ (log-scaled) \\
$\operatorname{batch\_size}$           & $\mathbb{N}^+$ & $\left[1, 1000 \right]$         \\ \bottomrule
\end{tabular}
\caption{Hyper-parameters to be optimized for logistic regression with SGD.}
\label{logistic_sgd}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Hyper-parameter} & \textbf{Type}                      & \textbf{Bounds}               \\ \midrule
$\operatorname{learning\_rate}$                & $\mathbb{R}^+$ & $\left[ 10^{-3}, 10^{-1} \right]$ (log-scaled) \\
$\operatorname{batch\_size}$           & $\mathbb{N}^+$ & $\left[1, 1000 \right]$         \\ 
$\operatorname{l_2\_reg}$		& $\mathbb{R}^+$ & $\left[ 10^{-4}, 10^{-2} \right]$ (log-scaled) \\ \bottomrule
\end{tabular}
\caption{Hyper-parameters to be optimized for MLP with SGD.}
\label{mlp_sgd}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Hyper-parameter} & \textbf{Type}                      & \textbf{Bounds}               \\ \midrule
$\operatorname{learning\_rate}$                & $\mathbb{R}^+$ & $\left[ 10^{-3}, 10^{-1} \right]$ (log-scaled) \\
$\operatorname{batch\_size}$           & $\mathbb{N}^+$ & $\left[1, 1000 \right]$         \\
$\operatorname{k_2}$           & $\mathbb{N}^+$ & $\left[10,  60 \right]$         \\
$\operatorname{k_1}$           & $\mathbb{N}^+$ & $\left[5,  k_2 \right]$         \\ \bottomrule
\end{tabular}
\caption{Hyper-parameters to be optimized for CNN with SGD.}
\label{cnn_sgd}
\end{table}

\paragraph{\textbf{Resource Allocation}} The type of resource considered here is the number of epochs, where one epoch means a pass of training through the whole training set using SGD. Note that this is similar to the original Hyperband paper where one unit of resources corresponds to 100 mini-batch iterations for example. One epoch may contain a various number of mini-batch iterations depending on the mini-batch size.

\paragraph{\textbf{Experimental Design}} In this section, we focus on neural network-typed classifiers, we will thus be maximizing the likelihood of the training set $D_{\operatorname{train}}$ under the model parameterized by $\theta$ (in this section, $\theta$ corresponds to $(W, b)$ where $W$ is the weight matrix and $b$ is the bias vector):
\[
\mathcal{L}(\theta, D_{\operatorname{train}}) = \frac{1}{|D_{\operatorname{train}}|} \sum_{i=1}^{|D_{\operatorname{train}}|} \log (\mathbb{P}(Y=y^{(i)}|x^{(i)},\theta),
\]
which is equivalent to minimize the loss function:
\[
\ell(\theta, D_{\operatorname{train}}) = -\mathcal{L}(\theta, D_{\operatorname{train}}).
\]

At each time step $t$, we give one unit of resources (one epoch of training here) to the current algorithm, who will run one epoch of training on the training set $D_{\operatorname{train}}$. The trained model is then used to predict output values $\hat{y}_{\operatorname{pred},t}$ and $\tilde{y}_{\operatorname{pred},t}$ respectively over validation set $D_{\operatorname{valid}}$ and test set $D_{\operatorname{test}}$. We then compute the number that were misclassified by the model, a.k.a. the zero-one loss on the validation and test set:
\[
\ell_t(\hat{\theta}_t, D_{\operatorname{valid}}) = \frac{1}{|D_{\operatorname{valid}}|} \sum_{i=1}^{|D_{\operatorname{valid}}|} \mathbb{1}_{\{\hat{y}_{\operatorname{pred},t}^{(i)} \neq y^{(i)}\}},
\]
\[
\ell_t(\hat{\theta}_t, D_{\operatorname{test}}) = \frac{1}{|D_{\operatorname{valid}}|} \sum_{i=1}^{|D_{\operatorname{valid}}|} \mathbb{1}_{\{\tilde{y}_{\operatorname{pred},t}^{(i)} \neq y^{(i)}\}}.
\]

During the experiment, we keep track of the best validation error and its associated test error. At each time step $t$, if the new validation error is smaller than the current best validation error, then we update the best validation error, and report the new test error. Otherwise we just report the test error associated with the previous best validation error.

Note that there is a very important 'keep training' notion here, which means if we are running the classifier with a same hyper-parameter configuration, then we do not restart the training from scratch. In contrary, we always keep track of previously trained weight matrix and bias vector with respect to the current hyper-parameter configuration, and train the model from these pre-trained parameters. 

The total budget for Hyperband would be $B = R\times s_{\max}$, and each configuration can be evaluted for $R_{\max}$ times (this $R_{\max}$ depends only on $R$ and $s_{\max}$). For HCT, we just need to feed the algorithm the total budget $B$, and the number of times that each configuration is evaluated will be decided by the algorithm itself. While for TPE, HOO and Random Search, we  evaluate each configuration for $R_{\max}$ times in order to make a fair comparison, which means $B/R_{\max}$ configurations will be evaluted.

\paragraph{\textbf{Comparison}}
All plots below are averaged on 10 trials of experiments.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/mnist/logistic_0.pdf}
    \caption{Performance (log-scale) comparison of different hyper-parameter optimization algorithms on Logistic Regression, trained on MNIST Dataset.}
    \label{logistic_0}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/mnist/mlp_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on MLP, trained on MNIST Dataset.}
    \label{mlp_0}
\end{figure}

\paragraph{\textbf{Discussion}} In the current setting, Hyperband seems to be a plausible choice since it may explore more configurations compared to other algorithms.


\section{UCI Datasets}

We consider here Adaptive Boosting (AdaBoost), Gradient Boosting Machine (GBM), k-Nearest Neighbors (KNN), Multi-Layer Perceptron (MLP), Support Vector Machine (SVM), Decision Tree and Random Forest from Scikit-learn.

\paragraph{\textbf{Dataset}} Several datasets on UCI dataset archive (e.g. Wine, Breast Cancer, etc) are being used. They are all pre-split into a training set $D_{\operatorname{train}}$ and a test set $D_{\operatorname{test}}$.

\paragraph{\textbf{Hyper-parameters}} The hyper-parameters to be optimized are listed below in Table \ref{adaparam}, \ref{gbmparam},  \ref{knnparam}, \ref{mlpparam}, \ref{svmparam}, \ref{treeparam} and \ref{rfparam}.

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter}             & \textbf{Type}  & \textbf{Bounds}                          \\ \midrule
\texttt{learning\_rate}      & $\mathbb{R}^+$ & $\left[10^{-5}, 10^{-1}\right]$                         \\
\texttt{n\_estimators}       & Integer        & $\left\lbrace 5,\dots, 200 \right\rbrace$
\end{tabular}
\caption{Hyper-parameters to be optimized for AdaBoost models.}
\label{adaparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter}             & \textbf{Type}  & \textbf{Bounds}                          \\ \midrule
\texttt{learning\_rate}      & $\mathbb{R}^+$ & $\left[10^{-5}, 10^{-2}\right]$                         \\
\texttt{n\_estimators}       & Integer        & $\left\lbrace 10,\dots, 100 \right\rbrace$ \\
\texttt{max\_depth}          & Integer        & $\left\lbrace 2, \dots, 100 \right\rbrace$ \\
\texttt{min\_samples\_split}  & Integer        & $\left\lbrace 2, \dots, 100 \right\rbrace$
\end{tabular}
\caption{Hyper-parameters to be optimized for GBM models.}
\label{gbmparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter} & \textbf{Type} & \textbf{Bounds}                           \\ \midrule
$k$                & Integer       & $\left\lbrace 10, \dots,50 \right\rbrace$
\end{tabular}
\caption{Hyper-parameters to be optimized for KNN models.}
\label{knnparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{lll}
\hline
\textbf{Parameter}             & \textbf{Type}    & \textbf{Bounds}       \\ \hline
\texttt{hidden\_layer\_size} & Integer          & $\left[5, 50\right]$  \\
\texttt{alpha}               & $\mathbb{R}^{+}$ & $\left[0, 0.9\right]$
\end{tabular}
\caption{Hyper-parameters to be optimized for MLP models.}
\label{mlpparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter} & \textbf{Type}                      & \textbf{Bounds}               \\ \midrule
$C$                & $\mathbb{R}^+$ & $\left[ 10^{-5}, 10^{5} \right]$ (log-scaled) \\
$\gamma$           & $\mathbb{R}^+$ & $\left[10^{-5}, 10^{5} \right]$  (log-scaled)       \\ \bottomrule
\end{tabular}
\caption{Hyper-parameters to be optimized for SVM models.}
\label{svmparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter}             & \textbf{Type}  & \textbf{Bounds}                          \\ \midrule
\texttt{max\_features}      & $\mathbb{R}^+$ & $\left[0.01, 0.99\right]$   \\
\texttt{max\_depth}          & Integer        & $\left\lbrace 4, \dots, 30 \right\rbrace$ \\
\texttt{min\_samples\_split}  & $\mathbb{R}^+$  & $\left[0.01, 0.99\right]$
\end{tabular}
\caption{Hyper-parameters to be optimized for Decision Tree models.}
\label{treeparam}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter}             & \textbf{Type}  & \textbf{Bounds}                          \\ \midrule
\texttt{max\_features}      & $\mathbb{R}^+$ & $\left[0.1, 0.5\right]$   \\
\texttt{n\_estimators}          & Integer        & $\left\lbrace 10, \dots, 50 \right\rbrace$ \\
\texttt{min\_samples\_split}  & $\mathbb{R}^+$  & $\left[0.1, 0.5\right]$
\end{tabular}
\caption{Hyper-parameters to be optimized for Random Forest models.}
\label{rfparam}
\end{table}

\paragraph{\textbf{Resource Allocation}} One unit of resources in this setting is one iteration of training, which means one complete training of each classifier/regressor over the whole training set.  

\paragraph{\textbf{Experimental Design}} In this section we use the logarithmic loss, also known as cross-entropy for classification problem, defined by:
\[
\ell(\theta, D) = - \frac{1}{|D|} \sum_{i=1}^{|D|} \sum_{j=1}^m y^{(i)}_{j}\log(\hat{p}^{(i)}_{j}),
\]
where $\hat{p}_{ij}$ is the predicted probability of a sample $i$ belonging to class $j$, and $m$ is the number of classes considered. And for regression problems, the loss that we use is the typical mean squared error, defined by:
\[
\ell(\theta, D) = - \frac{1}{|D|} \sum_{i=1}^{|D|} \left(y^{(i)} - \hat{y}^{(i)}_{\operatorname{pred}}\right)^2.
\]

And for this part of experiments, we choose to perform a shuffled $k=5$ cross-validation scheme on $D_{\operatorname{train}}$ at each time step $t$. In practice, this means that we fit $5$ models with the same architecture to different train/validation splits and average the loss results in each. More precisely, for every cross-validation split 
$\mathtt{cv}_j,j=1\ldots5$, we get a loss $\ell_{j,t}(\hat{\theta}_{j,t}, D_{\operatorname{valid},j,t}) = \dfrac{1}{n} \sum^{n}_{i=1} \left(y^{(i)}_{j} - \hat{y}^{(i)}_{\operatorname{pred},j,t}\right)^2$, where $n=|D_{\operatorname{valid}}|$ (here we take MSE as an example, it's the same for log-loss). Thus the validation loss at time $t$ is
\[
\frac{1}{5}\sum_{j=1}^{5}\ell_{j,t}(\hat{\theta}_{j,t}, D_{\operatorname{valid},j,t}) = \frac{1}{5n}\sum_{j=1}^{5} \sum_{i=1}^n \left(y^{(i)}_{j} - \hat{y}^{(i)}_{\operatorname{pred},j,t}\right)^2.
\]
Just like in the previous section, we can then compute and report the test error on the holdout test set $D_{\operatorname{test}}$:
\[
\ell_{t}(\hat{\theta}_t, D_{\operatorname{test}}) = \frac{1}{|D_{\operatorname{test}}|} \sum_{i=1}^{|D_{\operatorname{test}}|} \left(y^{(i)} - \tilde{y}^{(i)}_{\operatorname{pred},t}\right)^2.
\]

Note that under this experimental environment, 'keep training' does not make sense anymore. Thus for HOO, TPE and Random Search, we only need to evaluate each configuration once, contrarily to what we did in the previous setting. While for Hyperband, we still need to evaluate each configuration for a certain times based on $R$ and $s_{\max}$.

\paragraph{\textbf{Comparison}} Each plot here is averaged on 20 runs of experiments. Fig.~\ref{ada_0} to~\ref{svm_0} display the results for Wine Dataset.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/ada_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on AdaBoost, trained on Wine Dataset.}
    \label{ada_0}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/gbm_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on GBM, trained on Wine Dataset.}
    \label{gbm_0}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/knn_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on KNN, trained on Wine Dataset.}
    \label{knn_0}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/sk_mlp_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on MLP, trained on Wine Dataset.}
    \label{sk_mlp_0}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/svm_0.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on SVM, trained on Wine Dataset.}
    \label{svm_0}
\end{figure}

Fig.~\ref{ada_1} to~\ref{svm_1} display the results for Breast Cancer Dataset.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/ada_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on AdaBoost, trained on Breast Cancer Dataset.}
    \label{ada_1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/gbm_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on GBM, trained on Breast Cancer Dataset.}
    \label{gbm_1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/knn_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on KNN, trained on Breast Cancer Dataset.}
    \label{knn_1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/sk_mlp_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on MLP, trained on Breast Cancer Dataset.}
    \label{sk_mlp_1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.8]{img/uci/svm_1.pdf}
    \caption{Performance comparison of different hyper-parameter optimization algorithms on SVM, trained on Breast Cancer Dataset.}
    \label{svm_1}
\end{figure}

\paragraph{\textbf{Discussion}} In this setting, Hyperband seems to loose its advantage of exploring more configurations. It appears to me that there is no reason for HOO, TPE and Random Search to evaluate one point several times as does Hyperband.

\begin{thebibliography}{4}

\bibitem{li2016} Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., \& Talwalkar, A. (2016). Hyperband: A novel bandit-based approach to hyperparameter optimization. arXiv preprint arXiv:1603.06560.
\bibitem{bergstra2011} Bergstra, J. S., Bardenet, R., Bengio, Y., \& Kégl, B. (2011). Algorithms for hyper-parameter optimization. In Advances in neural information processing systems (pp. 2546-2554).
\bibitem{bubeck2011} Bubeck, S., Munos, R., Stoltz, G., \& Szepesvári, C. (2011). X-armed bandits. Journal of Machine Learning Research, 12(May), 1655-1695.
\bibitem{azar2014} Azar, M. G., Lazaric, A., \& Brunskill, E. (2014). Online stochastic optimization under correlated bandit feedback. In Proceedings of the 31st International Conference on Machine Learning (ICML-14) (pp. 1557-1565).
\bibitem{russo2016} Russo, D. (2016, June). Simple bayesian algorithms for best arm identification. In Conference on Learning Theory (pp. 1417-1418).

\end{thebibliography}


%\section*{Appendix}

\end{document}

