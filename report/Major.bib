Automatically generated by Mendeley Desktop 1.19.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{colson2007bilevel,
abstract = {This paper is devoted to bilevel optimization, a branch of mathematical programming of both practical and theoretical interest. Starting with a simple example, we proceed towards a general formulation. We then present fields of application, focus on solution approaches, and make the connection with MPECs (Mathematical Programs with Equilibrium Constraints).},
author = {Colson, Beno{\^{i}}t and Marcotte, Patrice and Savard, Gilles},
doi = {10.1007/s10479-007-0176-2},
isbn = {0254-5330},
issn = {02545330},
journal = {Annals of Operations Research},
keywords = {Bilevel programming,Mathematical programs with equilibrium constraints,Nonlinear programming,Optimal pricing},
number = {1},
pages = {235--256},
pmid = {4472240},
title = {{An overview of bilevel optimization}},
url = {https://www.iro.umontreal.ca/{~}marcotte/ARTIPS/AOR2007.pdf},
volume = {153},
year = {2007}
}
@inproceedings{auer1995,
abstract = {In the multi-armed bandit problem, a gambler must decide which arm$\backslash$nof K non-identical slot machines to play in a sequence of trials so as$\backslash$nto maximize his reward. This classical problem has received much$\backslash$nattention because of the simple model it provides of the trade-off$\backslash$nbetween exploration (trying out each arm to find the best one) and$\backslash$nexploitation (playing the arm believed to give the best payoff). Past$\backslash$nsolutions for the bandit problem have almost always relied on$\backslash$nassumptions about the statistics of the slot machines. In this work, we$\backslash$nmake no statistical assumptions whatsoever about the nature of the$\backslash$nprocess generating the payoffs of the slot machines. We give a solution$\backslash$nto the bandit problem in which an adversary, rather than a well-behaved$\backslash$nstochastic process, has complete control over the payoffs. In a sequence$\backslash$nof T plays, we prove that the expected per-round payoff of our algorithm$\backslash$napproaches that of the best arm at the rate O(T-1/3), and we$\backslash$ngive an improved rate of convergence when the best arm has fairly low$\backslash$npayoff. We also consider a setting in which the player has a team of$\backslash$n{\&}ldquo;experts{\&}rdquo; advising him on which arm to play; here, we give a$\backslash$nstrategy that will guarantee expected payoff close to that of the best$\backslash$nexpert. Finally, we apply our result to the problem of learning to play$\backslash$nan unknown repeated matrix game against an all-powerful adversary},
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert E.},
booktitle = {Proceedings of the 36th IEEE Annual Symposium on Foundations of Computer Science (FOCS)},
doi = {10.1109/SFCS.1995.492488},
isbn = {0-8186-7183-1},
issn = {0272-5428},
pages = {322--331},
title = {{Gambling in a rigged casino: The adversarial multi-armed bandit problem}},
url = {http://www.dklevine.com/archive/refs4462.pdf},
year = {1995}
}
@unpublished{kang2018,
author = {Kang, Qiyu and Tay, Wee Peng},
title = {{Task recommendation in crowdsourcing based on learning preferences and reliabilities}},
url = {https://arxiv.org/pdf/1807.10444.pdf}
}
@inproceedings{gerchinovitz2016,
abstract = {We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total lossof the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting.},
archivePrefix = {arXiv},
arxivId = {1605.07416},
author = {Gerchinovitz, S{\'{e}}bastien and Lattimore, Tor},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
eprint = {1605.07416},
issn = {10495258},
pages = {1198--1206},
title = {{Refined lower bounds for adversarial bandits}},
url = {http://arxiv.org/pdf/1605.07416.pdf},
year = {2016}
}
@article{dong2018,
abstract = {Information-theoretic Bayesian regret bounds of Russo and Van Roy capture the dependence of regret on prior uncertainty. However, this dependence is through entropy, which can become arbitrarily large as the number of actions increases. We establish new bounds that depend instead on a notion of rate-distortion. Among other things, this allows us to recover through information-theoretic arguments a near-optimal bound for the linear bandit. We also offer a bound for the logistic bandit that dramatically improves on the best previously available, though this bound depends on an information-theoretic statistic that we have only been able to quantify via computation.},
archivePrefix = {arXiv},
arxivId = {1805.11845},
author = {Dong, Shi and {Van Roy}, Benjamin},
eprint = {1805.11845},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Bayesian Optimization/An Information-Theoretic Analysis of Thompson Sampling with Many Actions.pdf:pdf},
issn = {15337928},
title = {{An information-theoretic analysis for Thompson Sampling with many actions}},
url = {http://arxiv.org/pdf/1805.11845.pdf},
year = {2018}
}
@inproceedings{sani2012risk,
abstract = {Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results.},
archivePrefix = {arXiv},
arxivId = {1301.1936},
author = {Sani, Amir and Lazaric, Alessandro and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS)},
eprint = {1301.1936},
isbn = {9781627480031},
issn = {10495258},
pages = {3275--3283},
title = {{Risk-aversion in multi-armed bandits}},
url = {http://arxiv.org/pdf/1301.1936.pdf},
year = {2012}
}
@inproceedings{kleinberg2004nearly,
abstract = {In the multiarmed bandit problem, an online algorithm must choose from a set of strategies in a sequence of n trials so as to minimize the total cost of the chosen strategies. While nearly tight upper and lower bounds are known in the case when the strategy set is finite, much less is known when there is an infinite strategy set. Here we consider the case when the set of strategies is a subset of R d , and the cost functions are continuous. In the d = 1 case, we improve on the bestknown upper and lower bounds, closing the gap to a sublogarithmic factor. We also consider the case where d {\textgreater} 1 and the cost functions are convex, adapting a recent online convex optimization algorithm of Zinkevich to the sparser feedback model of the multiarmed bandit problem.},
author = {Kleinberg, Robert},
booktitle = {Advances in Neural Information Processing Systems 17 (NIPS)},
isbn = {0262195348},
issn = {10495258},
pages = {697--704},
title = {{Nearly tight bounds for the continuum-armed bandit problem}},
url = {https://papers.nips.cc/paper/2634-nearly-tight-bounds-for-the-continuum-armed-bandit-problem.pdf},
year = {2004}
}
@article{banks1992,
author = {Banks, Jeffrey S. and Sundaram, Rangarajan K.},
journal = {Econometrica},
number = {5},
pages = {1071--1096},
title = {{Denumerable-armed bandits}},
url = {https://authors.library.caltech.edu/67326/1/2951539.pdf},
volume = {60},
year = {1992}
}
@inproceedings{jamieson2014lilucb,
author = {Jamieson, Kevin and Malloy, Matthew and Nowak, Robert and Bubeck, S{\'{e}}bastien},
booktitle = {Proceeding of the 27th Conference on Learning Theory (CoLT)},
pages = {423--439},
title = {{lil'UCB: An optimal exploration algorithm for multi-armed bandits}},
url = {https://arxiv.org/pdf/1312.7308.pdf},
year = {2014}
}
@inproceedings{jun2016atlucb,
abstract = {We introduce anytime Explore-m, a pure exploration problem for multi-armed bandits (MAB) that requires making a prediction of the top-m arms at every time step. Anytime Explore-m is more practical than fixed budget or fixed confidence formulations of the top-m problem, since many applications involve a finite, but unpredictable, budget. However, the development and analysis of anytime algorithms present many challenges. We propose AT-LUCB (AnyTime Lower and Upper Confidence Bound), the first nontrivial algorithm that provably solves anytime Explore-m. Our analysis shows that the sample complexity of AT-LUCB is competitive to anytime variants of existing algorithms. Moreover, our empirical evaluation on AT-LUCB shows that AT-LUCB performs as well as or better than state-of-the-art baseline methods for anytime Explore-m.},
author = {Jun, Kwang-Sung and Nowak, Robert},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},
isbn = {9781510829008},
pages = {974--982},
title = {{Anytime exploration for multi-armed bandits using confidence information}},
url = {http://proceedings.mlr.press/v48/jun16.pdf},
volume = {48},
year = {2016}
}
@article{chang2011libsvm,
abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multi-class classification, probability estimates, and parameter selection are discussed in detail},
author = {Chang, Chih-Chun and Lin, Chih-Jen},
doi = {10.1145/1961189.1961199},
isbn = {2157-6904},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
keywords = {classification,libsvm,optimization,regression,support vector ma-},
number = {3},
pages = {1--27},
pmid = {371},
title = {{LIBSVM: A library for support vector machines}},
url = {https://www.csie.ntu.edu.tw/{~}cjlin/papers/libsvm.pdf},
volume = {2},
year = {2011}
}
@unpublished{kapoor2018news,
archivePrefix = {arXiv},
arxivId = {1806.09202},
author = {Kapoor, Sayash and Keswani, Vijay and Vishnoi, Nisheeth K. and Celis, L. Elisa},
eprint = {1806.09202},
title = {{Balanced news using constrained bandit-based personalization}},
url = {http://arxiv.org/pdf/1806.09202.pdf},
year = {2018}
}
@article{lai1985,
abstract = {The authors consider multiarmed bandit problems with switching cost, define uniformly good allocation rules, and restrict attention to such rules. They present a lower bound on the asymptotic performance of uniformly good allocation rules and construct an allocation scheme that achieves the bound. It is found that despite the inclusion of a switching cost the proposed allocation scheme achieves the same asymptotic performance as the optimal rule for the bandit problem without switching cost. This is made possible by grouping together samples into blocks of increasing sizes, thereby reducing the number of switches to O(log {\textless}e1{\textgreater}n{\textless}/e1{\textgreater}). Finally, an optimal allocation scheme for a large class of distributions which includes members of the exponential family is illustrated},
author = {Lai, Tze-Leung and Robbins, Herbert},
doi = {10.1016/0196-8858(85)90002-8},
isbn = {0196-8858},
issn = {10902074},
journal = {Advances in Applied Mathematics},
number = {1},
pages = {4--22},
title = {{Asymptotically efficient adaptive allocation rules}},
url = {http://www.rci.rutgers.edu/{~}mnk/papers/Lai{\_}robbins85.pdf},
volume = {6},
year = {1985}
}
@inproceedings{li2017hyperband,
author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Talwalkar, Afshin Rostamizadeh Ameet},
booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
title = {{Hyperband: Bandit-based configuration evaluation for hyperparameter optimization}},
url = {https://openreview.net/pdf?id=ry18Ww5ee},
year = {2017}
}
@article{wang2017,
abstract = {Bayesian optimization (BO) has become an effective approach for black-box function optimization problems when function evaluations are expensive and the optimum can be achieved within a relatively small number of queries. However, many cases, such as the ones with high-dimensional inputs, may require a much larger number of observations for optimization. Despite an abundance of observations thanks to parallel experiments, current BO techniques have been limited to merely a few thousand observations. In this paper, we propose ensemble Bayesian optimization (EBO) to address three current challenges in BO simultaneously: (1) large-scale observations; (2) high dimensional input spaces; and (3) selections of batch queries that balance quality and diversity. The key idea of EBO is to operate on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer. We show unprecedented, previously impossible results of scaling up BO to tens of thousands of observations within minutes of computation.},
archivePrefix = {arXiv},
arxivId = {1706.01445},
author = {Wang, Zi and Gehring, Clement and Kohli, Pushmeet and Jegelka, Stefanie},
eprint = {1706.01445},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Bayesian Optimization/Batched Large-scale Bayesian Optimization in High-dimensional Spaces.pdf:pdf},
title = {{Batched large-scale Bayesian optimization in high-dimensional spaces}},
url = {http://arxiv.org/pdf/1706.01445.pdf},
volume = {84},
year = {2017}
}
@article{scarlett2018,
abstract = {We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time {\$}T{\$} behaves as {\$}\backslashOmega(\backslashsqrt{\{}T{\}}){\$} and {\$}O(\backslashsqrt{\{}T\backslashlog T{\}}){\$}. This gives a tight characterization up to a {\$}\backslashsqrt{\{}\backslashlog T{\}}{\$} factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Mat$\backslash$'ern-{\$}\backslashnu{\$} kernels, with the latter requiring {\$}\backslashnu {\textgreater} 2{\$}. Our results certify the near-optimality of existing bounds (Srinivas {\{}$\backslash$em et al.{\}}, 2009) for the SE kernel, while proving them to be strictly suboptimal for the Mat$\backslash$'ern kernel with {\$}\backslashnu {\textgreater} 2{\$}.},
archivePrefix = {arXiv},
arxivId = {1805.11792},
author = {Scarlett, Jonathan},
eprint = {1805.11792},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Bayesian Optimization/Tight Regret Bounds for Bayesian Optimization in One Dimension.pdf:pdf},
title = {{Tight regret bounds for Bayesian optimization in one dimension}},
url = {http://arxiv.org/pdf/1805.11792.pdf},
year = {2018}
}
@inproceedings{li2010contextual,
abstract = {Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5{\%} click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.},
archivePrefix = {arXiv},
arxivId = {1003.0146},
author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
booktitle = {Proceedings of the 19th International World Wide Web Conference (WWW)},
doi = {10.1145/1772690.1772758},
eprint = {1003.0146},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Contextual Bandit/A Contextual-Bandit Approach to Personalized News Article Recommendation.pdf:pdf},
isbn = {9781605587998},
issn = {9781605587998},
keywords = {contextual bandit,exploitation dilemma,exploration,personalization,recommender sys,tems,web service},
organization = {ACM},
pages = {661--670},
publisher = {ACM Press},
title = {{A contextual-bandit approach to personalized news article recommendation}},
url = {https://arxiv.org/pdf/1003.0146.pdf},
year = {2010}
}
@article{joulani2017,
abstract = {Recently, much work has been done on extending the scope of online learning and incremental stochastic optimization algorithms. In this paper we contribute to this effort in two ways: First, based on a new regret decomposition and a generalization of Bregman divergences, we provide a self-contained, modular analysis of the two workhorses of online learning: (general) adaptive versions of Mirror Descent (MD) and the Follow-the-Regularized-Leader (FTRL) algorithms. The analysis is done with extra care so as not to introduce assumptions not needed in the proofs and allows to combine, in a straightforward way, different algorithmic ideas (e.g., adaptivity, optimism, implicit updates) and learning settings (e.g., strongly convex or composite objectives). This way we are able to reprove, extend and refine a large body of the literature, while keeping the proofs concise. The second contribution is a byproduct of this careful analysis: We present algorithms with improved variational bounds for smooth, composite objectives, including a new family of optimistic MD algorithms with only one projection step per round. Furthermore, we provide a simple extension of adaptive regret bounds to practically relevant non-convex problem settings with essentially no extra effort.},
archivePrefix = {arXiv},
arxivId = {1709.02726},
author = {Joulani, Pooria and Gy{\"{o}}rgy, Andr{\'{a}}s and Szepesv{\'{a}}ri, Csaba},
eprint = {1709.02726},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Online Learning/A Modular Analysis of Adaptive (Non-)Convex Optimization$\backslash$: Optimism, Composite Objectives, and Variational Bounds.pdf:pdf},
journal = {Proceedings of the 28th International Conference on Algorithmic Learning Theory (ALT)},
keywords = {ada-,follow-the-regularized-leader,grad,implicit updates,mirror-descent,non-convex optimization,online learning,optimistic online,stochastic optimization},
title = {{A modular analysis of adaptive (non-)convex optimization: Optimism, composite objectives, and variational bounds}},
url = {https://arxiv.org/pdf/1709.02726.pdf},
year = {2017}
}
@inproceedings{hutter2011smac,
abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.},
author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
booktitle = {Proceedings of the 5th International Conference on Learning and Intelligent Optimization (LION)},
doi = {10.1007/978-3-642-25566-3_40},
isbn = {9783642255656},
issn = {03029743},
pages = {507--523},
title = {{Sequential model-based optimization for general algorithm configuration}},
url = {https://www.cs.ubc.ca/{~}hutter/papers/10-TR-SMAC.pdf},
year = {2011}
}
@inproceedings{garivier2011klucb,
abstract = {This paper presents a finite-time analysis of the KL-UCB algorithm, an online, horizon-free index policy for stochastic bandit problems. We prove two distinct results: first, for arbitrary bounded rewards, the KL-UCB algorithm satisfies a uniformly better regret bound than UCB or UCB2; second, in the special case of Bernoulli rewards, it reaches the lower bound of Lai and Robbins. Furthermore, we show that simple adaptations of the KL-UCB algorithm are also optimal for specific classes of (possibly unbounded) rewards, including those generated from exponential families of distributions. A large-scale numerical study comparing KL-UCB with its main competitors (UCB, UCB2, UCB-Tuned, UCB-V, DMED) shows that KL-UCB is remarkably efficient and stable, including for short time horizons. KL-UCB is also the only method that always performs better than the basic UCB policy. Our regret bounds rely on deviations results of independent interest which are stated and proved in the Appendix. As a by-product, we also obtain an improved regret bound for the standard UCB algorithm.},
archivePrefix = {arXiv},
arxivId = {1102.2490},
author = {Garivier, Aur{\'{e}}lien and Capp{\'{e}}, Olivier},
booktitle = {Proceeding of the 25th Conference on Learning Theory (CoLT)},
eprint = {1102.2490},
issn = {15337928},
title = {{The KL-UCB algorithm for bounded stochastic bandits and beyond}},
url = {http://arxiv.org/pdf/1102.2490.pdf},
year = {2011}
}
@inproceedings{falkner2018bohb,
abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
archivePrefix = {arXiv},
arxivId = {1807.01774},
author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
eprint = {1807.01774},
title = {{BOHB: Robust and Efficient Hyperparameter Optimization at Scale}},
url = {https://arxiv.org/pdf/1807.01774.pdf},
year = {2018}
}
@article{preux2014bandits,
abstract = {We consider function optimization as a sequential decision making problem under the budget constraint. Such constraint limits the number of objective function evaluations allowed during the optimization. We consider an algorithm inspired by a continuous version of a multi-armed bandit problem which attacks this optimization problem by solving the tradeoff between exploration (initial quasi-uniform search of the domain) and exploitation (local optimization around the potentially global maxima). We introduce the so-called Simultaneous Optimistic Optimization (SOO), a deterministic algorithm that works by domain partitioning. The benefit of such an approach are the guarantees on the returned solution and the numerical eficiency of the algorithm. We present this machine learning rooted approach to optimization, and provide the empirical assessment of SOO on the CEC'2014 competition on single objective real-parameter numerical optimization testsuite.},
author = {Preux, Philippe and Munos, R{\'{e}}mi and Valko, Michal},
file = {:home/xuedong/Documents/xuedong/phd/work/references/Major/Bandit Theory/Continuum-armed Bandits/Bandits Attack Function Optimization.pdf:pdf},
journal = {IEEE Congress on Evolutionary Computation},
title = {{Bandits attack function optimization}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=6900558},
year = {2014}
}
@article{wang2007,
abstract = {Computation-intensive design problems are becoming increasingly common in manufacturing industries. The computation burden is often caused by expensive analysis and simulation processes in order to reach a comparable level of accuracy as physical testing data. To address such a challenge, approximation or metamodeling techniques are often used. Metamodeling techniques have been developed from many different disciplines including statistics, mathematics, computer science, and various engineering disciplines. These metamodels are initially developed as “surrogates” of the expensive simulation process in order to improve the overall computation efficiency. They are then found to be a valuable tool to support a wide scope of activities in modern engineering design, especially design optimization. This work reviews the state-of-the-art metamodel-based techniques from a practitioner's perspective according to the role of metamodeling in supporting design optimization, including model approximation, design space exploration, problem formulation, and solving various types of optimization problems. Challenges and future development of metamodeling in support of engineering design is also analyzed and discussed.},
author = {Wang, Gary G. and Shan, Songqing},
doi = {10.1115/1.2429697},
isbn = {0-7918-4255-X},
issn = {10500472},
journal = {Journal of Mechanical Design},
number = {4},
pages = {370},
title = {{Review of metamodeling techniques in support of engineering design optimization}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?articleid=1449318},
volume = {129},
year = {2007}
}
@inproceedings{kocsis2006bandit,
abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
author = {Kocsis, Levente and Szepesv{\'{a}}ri, Csaba},
booktitle = {Proceedings of the 17th European Conference on Machine Learning (ECML)},
keywords = {bandits},
mendeley-tags = {bandits},
title = {{Bandit-based Monte-Carlo planning}},
url = {http://ggp.stanford.edu/readings/uct.pdf},
year = {2006}
}
@inproceedings{munos2011soo,
abstract = {We consider a global optimization problem of a deterministic function f in a semi-metric space, given a ﬁnite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric. We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A ﬁrst contribution is an algorithm, DOO, that requires the knowledge of . We report a ﬁnite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then deﬁne a second algorithm, SOO, which does not require the knowledge of the semi-metric under which f is smooth, and whose performance is almost as good as DOO optimally-ﬁtted.},
author = {Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 24 (NIPS)},
file = {:home/xuedong/Documents/xuedong/phd/work/references/Major/Bandit Theory/Continuum-armed Bandits/Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness.pdf:pdf},
isbn = {9781618395993},
pages = {783--791},
title = {{Optimistic optimization of deterministic functions without the knowledge of its smoothness}},
url = {https://papers.nips.cc/paper/4304-optimistic-optimization-of-a-deterministic-function-without-the-knowledge-of-its-smoothness.pdf},
year = {2011}
}
@inproceedings{bonald2013bernoulli,
author = {Bonald, Thomas and Prouti{\`{e}}re, Alexandre},
booktitle = {Advances in Neural Information Processing Systems 26 (NIPS)},
issn = {10495258},
pages = {2184--2192},
title = {{Two-target algorithms for infinite-armed bandits with Bernoulli rewards}},
url = {https://papers.nips.cc/paper/5109-two-target-algorithms-for-infinite-armed-bandits-with-bernoulli-rewards.pdf},
year = {2013}
}
@inproceedings{li2017ucbglm,
abstract = {Contextual bandits are widely used in Internet services from news recommendation to advertising, and to Web search. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an {\$}\backslashtilde{\{}O{\}}(\backslashsqrt{\{}dT{\}}){\$} regret over {\$}T{\$} rounds with {\$}d{\$} dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a {\$}\backslashsqrt{\{}d{\}}{\$} factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for certain cases.},
archivePrefix = {arXiv},
arxivId = {1703.00048},
author = {Li, Lihong and Lu, Yu and Zhou, Dengyong},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
eprint = {1703.00048},
isbn = {9781510855144},
pages = {2071--2080},
title = {{Provably optimal algorithms for generalized linear contextual bandits}},
url = {http://arxiv.org/pdf/1703.00048.pdf},
year = {2017}
}
@inproceedings{agrawal2013further,
abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that simultaneously proves both the optimal problem-dependent bound of {\$}(1+\backslashepsilon)\backslashsum{\_}i \backslashfrac{\{}\backslashln T{\}}{\{}\backslashDelta{\_}i{\}}+O(\backslashfrac{\{}N{\}}{\{}\backslashepsilon{\^{}}2{\}}){\$} and the first near-optimal problem-independent bound of {\$}O(\backslashsqrt{\{}NT\backslashln T{\}}){\$} on the expected regret of this algorithm. Our near-optimal problem-independent bound solves a COLT 2012 open problem of Chapelle and Li. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are conceptually simple, easily extend to distributions other than the Beta distribution, and also extend to the more general contextual bandits setting [Manuscript, Agrawal and Goyal, 2012].},
archivePrefix = {arXiv},
arxivId = {1209.3353},
author = {Agrawal, Shipra and Goyal, Navin},
booktitle = {Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AIStats)},
doi = {10.1145/3088510},
eprint = {1209.3353},
issn = {15337928},
pages = {99--107},
title = {{Further optimal regret bounds for Thompson Sampling}},
url = {http://arxiv.org/pdf/1209.3353.pdf},
year = {2012}
}
@book{ziemba2010,
author = {Ziemba, William T. and Vickson, Raymond G.},
booktitle = {The Journal of Risk and Insurance},
issn = {1095-8541},
pmid = {21126524},
title = {{Stochastic Optimization Models in Finance}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1539-6975.2010.01374.x},
year = {2010}
}
@inproceedings{jamieson2016hyperband,
abstract = {Motivated by the task of hyperparameter optimization, we introduce the non-stochastic best-arm identification problem. Within the multi-armed bandit literature, the cumulative regret objective enjoys algorithms and analyses for both the non-stochastic and stochastic settings while to the best of our knowledge, the best-arm identification framework has only been considered in the stochastic setting. We introduce the non-stochastic setting under this framework, identify a known algorithm that is well-suited for this setting, and analyze its behavior. Next, by leveraging the iterative nature of standard machine learning algorithms, we cast hyperparameter optimization as an instance of non-stochastic best-arm identification, and empirically evaluate our proposed algorithm on this task. Our empirical results show that, by allocating more resources to promising hyperparameter settings, we typically achieve comparable test accuracies an order of magnitude faster than baseline methods.},
archivePrefix = {arXiv},
arxivId = {1502.07943},
author = {Jamieson, Kevin and Talwalkar, Ameet},
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {1502.07943},
title = {{Non-stochastic best arm identification and hyperparameter optimization}},
url = {http://arxiv.org/pdf/1502.07943.pdf},
year = {2016}
}
@article{auer2002exp3,
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Freund, Yoav and Schapire, Robert E.},
journal = {SIAM Journal of Computing},
pages = {48--77},
title = {{The nonstochastic multiarmed bandit problem}},
url = {http://rob.schapire.net/papers/AuerCeFrSc01.pdf},
volume = {32},
year = {2002}
}
@unpublished{kleinberg2013,
abstract = {In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions. In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the "Lipschitz MAB problem". We present a solution for the multi-armed bandit problem in this setting. That is, for every metric space we define an isometry invariant which bounds from below the performance of Lipschitz MAB algorithms for this metric space, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions. We also address the full-feedback ("best expert") version of the problem, where after every round the payoffs from all arms are revealed.},
archivePrefix = {arXiv},
arxivId = {1312.1277},
author = {Kleinberg, Robert and Slivkins, Aleksandrs and Upfal, Eli},
eprint = {1312.1277},
title = {{Bandits and experts in metric spaces}},
url = {http://arxiv.org/pdf/1312.1277.pdf},
year = {2013}
}
@inproceedings{abbasi-yadkori2018best,
abstract = {We study bandit best-arm identification with arbitrary and potentially adversarial rewards. A simple random uniform learner obtains the optimal rate of error in the adversarial scenario. However, this type of strategy is suboptimal when the rewards are sampled stochastically. Therefore, we ask: Can we design a learner that performs optimally in both the stochastic and adversarial problems while not being aware of the nature of the rewards? First, we show that designing such a learner is impossible in general. In particular, to be robust to adversarial rewards, we can only guarantee optimal rates of error on a subset of the stochastic problems. We give a lower bound that characterizes the optimal rate in stochastic problems if the strategy is constrained to be robust to adversarial rewards. Finally, we design a simple parameter-free algorithm and show that its probability of error matches (up to log factors) the lower bound in stochastic problems, and it is also robust to adversarial ones.},
author = {Abbasi-Yadkori, Yasin and Bartlett, Peter and Gabillon, Victor and Malek, Alan and Valko, Michal},
booktitle = {Proceedings of the 31st Conference on Learning Theory (CoLT)},
title = {{Best of both worlds: Stochastic {\&} adversarial best-arm identification}},
url = {http://researchers.lille.inria.fr/{~}valko/hp/publications/abbasi-yadkori2018best.pdf},
year = {2018}
}
@inproceedings{langford2008epoch,
author = {Langford, John and Zhang, Tong},
booktitle = {Advances in Neural Information Processing Systems 20 (NIPS)},
pages = {817--824},
title = {{The Epoch-Greedy algorithm for multi-armed bandits with side information}},
url = {http://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information.pdf},
year = {2008}
}
@techreport{li2016,
abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with state-of-the-art methods on a suite of hyperparameter optimization problems. We observe that Hyperband provides five times to thirty times speedup over state-of-the-art Bayesian optimization algorithms on a variety of deep-learning and kernel-based learning problems.},
archivePrefix = {arXiv},
arxivId = {1603.06560},
author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
eprint = {1603.06560},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - Hyperband A Novel Bandit-Based Approach to Hyperparameter Optimization.pdf:pdf},
month = {mar},
title = {{Hyperband: A novel bandit-based approach to hyperparameter optimization}},
url = {http://arxiv.org/pdf/1603.06560.pdf},
year = {2016}
}
@inproceedings{valko2013stosoo,
abstract = {We study the problem of global maximization of a function f given a finite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, StoSOO, follows an optimistic strategy to iteratively construct upper confidence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A finite-time analysis of StoSOO shows that it performs almost as well as the best specifically-tuned algorithms even though the local smoothness of the function is not known.},
author = {Valko, Michal and Carpentier, Alexandra and Munos, R{\'{e}}mi},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/Documents/xuedong/phd/work/references/Major/Bandit Theory/Continuum-armed Bandits/Stochastic Simultaneous Optimistic Optimization.pdf:pdf},
pages = {19--27},
title = {{Stochastic simultaneous optimistic optimization}},
url = {http://proceedings.mlr.press/v28/valko13.pdf},
year = {2013}
}
@inproceedings{even-dar2003confidence,
abstract = {We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O((n/$\epsilon$2)log(1/$\delta$)) times to find an $\epsilon$-optimal arm with probability of at least 1-$\delta$. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over $\epsilon$-greedy Q-learning.},
author = {Even-dar, Eyal and Mannor, Shie and Mansour, Yishay},
booktitle = {Proceedings of the 20th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Best Arm Identification/Action Elimination and Stopping Conditions for the Multi-armed Bandit and Reinforcement Learning Problems.pdf:pdf},
isbn = {1577351894},
issn = {15324435},
pages = {162--169},
title = {{Action elimination and stopping conditions for reinforcement learning}},
url = {https://www.aaai.org/Papers/ICML/2003/ICML03-024.pdf},
year = {2003}
}
@book{murphy2012,
author = {Murphy, Kevin P.},
publisher = {The MIT Press},
title = {{Machine Learning: A Probabilistic Perspective}},
url = {https://mitpress.mit.edu/books/machine-learning-1},
year = {2012}
}
@unpublished{zhao2018offloading,
author = {Zhao, Shangshu and Zhu, Zhaowei and Yang, Fuqian and Luo, Xiliang},
title = {{Online optimal task offloading with one-bit feedback}},
url = {https://arxiv.org/pdf/1806.10547.pdf},
year = {2018}
}
@inproceedings{korda2013thompson,
abstract = {Thompson Sampling has been demonstrated in many complex bandit models, however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the Bernoulli case. Here we extend them by proving asymptotic optimality of the algorithm using the Jeffreys prior for 1-dimensional exponential family bandits. Our proof builds on previous work, but also makes extensive use of closed forms for Kullback-Leibler divergence and Fisher information (and thus Jeffreys prior) available in an exponential family. This allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right. Moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed, including heavy-tailed exponential families.},
archivePrefix = {arXiv},
arxivId = {1307.3400},
author = {Korda, Nathaniel and Kaufmann, Emilie and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
eprint = {1307.3400},
issn = {10495258},
pages = {1448--1456},
title = {{Thompson Sampling for 1-dimensional exponential family bandits}},
url = {http://arxiv.org/pdf/1307.3400.pdf},
year = {2013}
}
@inproceedings{chu2011linucb,
abstract = {Abstract In this paper we study the contextual bandit problem (also known as the multi- armed bandit problem with expert advice) for linear payoff functions . For T rounds, K actions, and d dimensional feature vectors, we prove an O$\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1106.2369v1},
author = {Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert E.},
booktitle = {Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AIStats)},
eprint = {arXiv:1106.2369v1},
issn = {{\textless}null{\textgreater}},
keywords = {To Read Urgently},
pages = {208--214},
title = {{Contextual bandits with linear payoff functions}},
url = {https://www.cs.princeton.edu/{~}schapire/papers/bandit-lin.pdf{\%}5Cnpapers2://publication/uuid/03F8E2D2-621B-414E-A5DE-3B5E5659E4DD{\%}5Cnhttp://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2011{\_}ChuLRS11.pdf},
year = {2011}
}
@unpublished{raghavan2018,
abstract = {Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users for information that will lead to better decisions in the future. Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups. Motivated by these concerns, we initiate the study of the externalities of exploration - the undesirable side effects that the presence of one party may impose on another - under the linear contextual bandits model. We introduce the notion of a group externality, measuring the extent to which the presence of one population of users impacts the rewards of another. We show that this impact can in some cases be negative, and that, in a certain sense, no algorithm can avoid it. We then study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal, improving on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most {\$}\backslashtilde{\{}O{\}}(T{\^{}}{\{}1/3{\}}){\$}. Returning to group-level effects, we show that under the same conditions, negative group externalities essentially vanish under the greedy algorithm. Together, our results uncover a sharp contrast between the high externalities that exist in the worst case, and the ability to remove all externalities if the data is sufficiently diverse.},
archivePrefix = {arXiv},
arxivId = {1806.00543},
author = {Raghavan, Manish and Slivkins, Aleksandrs and Vaughan, Jennifer W. and Wu, Zhiwei S.},
eprint = {1806.00543},
title = {{The externalities of exploration and how data diversity helps exploitation}},
url = {http://arxiv.org/pdf/1806.00543.pdf},
year = {2018}
}
@inproceedings{bergstra2011tpe,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap-proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos-sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu-ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex-pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli-able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
author = {Bergstra, James and Bardenet, R{\'{e}}mi and Bengio, Yoshua and K{\'{e}}gl, Bal{\'{a}}zs},
booktitle = {Advances in Neural Information Processing Systems 24 (NIPS)},
pages = {2546--2554},
title = {{Algorithms for hyper-parameter optimization}},
url = {https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
year = {2011}
}
@unpublished{brochu2010bayesian,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
doi = {10.1007/9783642532580?COVERIMAGEURL=HTTPS://STATICCONTENT.SPRINGER.COM/COVER/BOOK/9783642532580.JPG},
eprint = {1012.2599},
isbn = {0671631985},
issn = {09420940},
pmid = {27489955},
title = {{A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning}},
url = {http://arxiv.org/pdf/1012.2599.pdf},
year = {2010}
}
@inproceedings{grill2016trail,
abstract = {We study the sampling-based planning problem in Markov decision processes (MDPs) that we can access only through a generative model, usually referred to as Monte-Carlo planning. Our objective is to return a good estimate of the optimal value function at any state while minimizing the number of calls to the generative model, i.e. the sample complexity. We propose a new algorithm, TrailBlazer, able to handle MDPs with a finite or an infinite number of transitions from state-action to next states. TrailBlazer is an adaptive algorithm that exploits possible structures of the MDP by exploring only a subset of states reachable by following near-optimal policies. We provide bounds on its sample complexity that depend on a measure of the quantity of near-optimal states. The algorithm behavior can be considered as an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). Finally, another appealing feature of TrailBlazer is that it is simple to implement and computationally efficient.},
author = {Grill, Jean-Bastien and Valko, Michal and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
issn = {10495258},
number = {Nips},
pages = {4680--4688},
title = {{Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning}},
url = {https://papers.nips.cc/paper/6253-blazing-the-trails-before-beating-the-path-sample-efficient-monte-carlo-planning.pdf},
year = {2016}
}
@inproceedings{filippi2010glmucb,
abstract = {We consider structured multi-armed bandit problems based on the Generalized Linear Model (GLM) framework of statistics. For these bandits, we propose a new algorithm, called GLM-UCB. We derive finite time, high probability bounds on the regret of the algorithm, extending previous analyses developed for the linear bandits to the non-linear case. The analysis highlights a key difficulty in generalizing linear bandit algorithms to the non-linear case, which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual effectiveness of current parameterized bandit algorithms is often poor in practice, we provide a tuning method based on asymptotic arguments, which leads to significantly better practical performance. We present two numerical experiments on real-world data that illustrate the potential of the GLM-UCB approach.$\backslash$n},
author = {Filippi, Sarah and Capp{\'{e}}, Olivier and Garivier, Aur{\'{e}}lien and Szepesv{\'{a}}ri, Csaba},
booktitle = {Advances in Neural Information Processing Systems 22 (NIPS)},
isbn = {9781617823800},
keywords = {generalized linear models,multi-armed bandit,parametric bandits,regret minimization,ucb},
pages = {586--594},
title = {{Parametric bandits: The generalized linear case}},
url = {https://papers.nips.cc/paper/4166-parametric-bandits-the-generalized-linear-case.pdf},
year = {2010}
}
@inproceedings{gabillon2012ugape,
abstract = {We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to a meta-algorithm called unified gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms.},
author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
booktitle = {Advances in Neural Information Processing Systems 25 (NIPS)},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Best Arm Identification/Best Arm Identification$\backslash$: A Unified Approach to Fixed Budget and Fixed Confidence.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
pages = {3212--3220},
title = {{Best arm identification: A unified approach to fixed budget and fixed confidence}},
url = {http://papers.nips.cc/paper/4640-best-arm-identification-a-unified-approach-to-fixed-budget-and-fixed-confidence.pdf},
year = {2012}
}
@unpublished{thune2018soda,
author = {Thune, Tobias S. and Seldin, Yevgeny},
title = {{Adaptation to easy data in prediction with limited advice}},
url = {https://arxiv.org/pdf/1807.00636.pdf},
year = {2018}
}
@book{cover2006,
author = {Cover, Thomas M. and Thomas, Joy A.},
isbn = {9780387781891},
issn = {08848289},
publisher = {John Wiley {\&} Sons, Inc},
title = {{Elements of Information Theory}},
url = {http://staff.ustc.edu.cn/{~}cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf},
year = {2006}
}
@article{hinz2018speeding,
abstract = {Most modern learning algorithms require the practitioner to manually set the values of many hyperparameters before the learning process can begin. However, with modern algo-rithms the evaluation of a given hyperparameter setting can take a considerable amount of time and the search space is often very high-dimensional. We propose the usage of a genetic algorithm to optimize the hyperparameters specifically for convolutional neural networks (CNNs). Additionally, we suggest using a lower-dimensional representation of the original data to quickly identify promising areas in the hyperparameter space. We compare the re-sults of the genetic algorithm and the hyperparameter optimization on lower-dimensional inputs to random search and the " Tree of Parzen Estimators " (TPE) algorithm. Our experiments show that the genetic algorithm finds hyperparameters that perform similar or better than those found by random search and the TPE algorithm. Additionally, they indicate that it is possible to speed up the optimization process by using lower-dimensional data representations at the beginning while increasing the dimensionality of the input later in the optimization process. This is independent of the underlying optimization procedure and considerably speeds up the hyperparameter optimization process, making the approach promising for future hyperparameter optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1807.07362},
author = {Hinz, Tobias and Navarro-Guerrero, Nicol{\'{a}}s and Magg, Sven and Wermter, Stefan},
doi = {10.1142/S1469026818500086},
eprint = {1807.07362},
issn = {14690268},
journal = {International Journal of Computational Intelligence and Applications},
keywords = {convolutional neural networks,evolutionary algorithm,genetic algorithm,hyperparameter importance,hyperparameter optimization},
number = {2},
title = {{Speeding up the hyperparameter optimization of deep convolutional neural networks}},
url = {https://arxiv.org/pdf/1807.07362.pdf},
volume = {17},
year = {2018}
}
@book{berry1985,
author = {Berry, Donald A. and Fristedt, Bert},
publisher = {Springer Science {\&} Business Media},
title = {{Bandit Problems: Sequential Allocations of Experiments}},
url = {https://link.springer.com/content/pdf/10.1007/978-94-015-3711-7.pdf},
year = {1985}
}
@inproceedings{azar2014online,
abstract = {In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel any-time {\$}\backslashmathcal{\{}X{\}}{\$}-armed bandit algorithm, and derive regret bounds matching the performance of existing state-of-the-art in terms of dependency on number of steps and smoothness factor. The main advantage of HCT is that it handles the challenging case of correlated rewards, whereas existing methods require that the reward-generating process of each arm is an identically and independent distributed (iid) random process. HCT also improves on the state-of-the-art in terms of its memory requirement as well as requiring a weaker smoothness assumption on the mean-reward function in compare to the previous anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.},
author = {Azar, Mohammad Gheshlaghi and Lazaric, Alessandro and Brunskill, Emma},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML)},
file = {:home/xuedong/Documents/xuedong/phd/work/references/Major/Bandit Theory/Continuum-armed Bandits/Online Stochastic Optimization under Correlated Bandit Feedback.pdf:pdf},
pages = {1557--1565},
title = {{Online stochastic optimization under correlated bandit feedback}},
url = {https://arxiv.org/pdf/1402.0562.pdf},
year = {2014}
}
@inproceedings{karnin2013sha,
abstract = {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplica-tive gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale ap-plications. We present two novel, parameter-free algorithms for identifying the best arm, in two different settings: given a target con-fidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doubly-logarithmic in the problem parameters. We corroborate our theoretical results with ex-periments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.},
author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Best Arm Identification/Almost Optimal Exploration in Multi-Armed Bandits.pdf:pdf},
pages = {1238--1246},
title = {{Almost optimal exploration in multi-armed bandits}},
url = {http://proceedings.mlr.press/v28/karnin13.pdf},
year = {2013}
}
@book{gittins2011,
abstract = {In 1989 the first edition of this book set out Gittins' pioneering index solution to the multi-armed bandit problem and his subsequent investigation of a wide class of sequential resource allocation and stochastic scheduling problems. Since then there has been a remarkable flowering of new insights, generalizations and applications, to which Glazebrook and Weber have made major contributions. This second edition brings the story up to date. There are new chapters on the achievable region approach to stochastic optimization problems, the construction of performance bounds for suboptimal policies, Whittle's restless bandits, and the use of Lagrangian relaxation in the construction and evaluation of index policies. Some of the many varied proofs of the index theorem are discussed along with the insights that they provide. Many contemporary applications are surveyed, and over 150 new references are included. Over the past 40 years the Gittins index has helped theoreticians and practitioners to address a huge variety of problems within chemometrics, economics, engineering, numerical analysis, operational research, probability, statistics and website design. This new edition will be an important resource for others wishing to use this approach.},
author = {Gittins, John and Glazebrook, Kevin and Weber, Richard},
doi = {10.1057/jors.1989.200},
edition = {2nd},
isbn = {9780470670026},
issn = {14769360},
publisher = {John Wiley {\&} Sons, Inc},
title = {{Multi-armed Bandit Allocation Indices}},
url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470980033},
year = {2011}
}
@inproceedings{wang2008ucbv,
abstract = {We consider multi-armed bandit problems where the number of arms is larger than the possible number of experiments. We make a stochastic assumption on the mean-reward of a new selected arm which characterizes its probability of being a near-optimal arm. Our assumption is weaker than in previous works. We describe algorithms based on upper-confidence-bounds applied to a restricted set of randomly selected arms and provide upper-bounds on the resulting expected regret. We also derive a lower-bound which matches (up to a logarithmic factor) the upper-bound in some cases.},
author = {Wang, Yizao and Audibert, Jean-Yves and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 21 (NIPS)},
isbn = {9781605609492},
keywords = {Computational,Information-Theoretic Learning with Statistics,Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {1729--1736},
title = {{Algorithms for infinitely many-armed bandits}},
url = {https://papers.nips.cc/paper/3452-algorithms-for-infinitely-many-armed-bandits.pdf},
year = {2008}
}
@article{auer2002ucb,
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Fischer, Paul},
journal = {Machine Learning Journal},
number = {2–3},
pages = {235--256},
title = {{Finite-time analysis of the multi- armed bandit problem}},
url = {https://homes.di.unimi.it/{~}cesabian/Pubblicazioni/ml-02.pdf},
volume = {47},
year = {2002}
}
@inproceedings{audibert2010budget,
abstract = {We consider the problem of finding the best arm in a stochastic multi-armed bandit game. The regret of a forecaster is here defined by the gap between the mean reward of the optimal arm and the mean reward of the ultimately chosen arm. We propose a highly exploring UCB policy and a new algorithm based on successive rejects. We show that these algorithms are essentially optimal since their regret decreases exponentially at a rate which is, up to a logarithmic factor, the best possible. However, while the UCB policy needs the tuning of a parameter depending on the unobservable hardness of the task, the successive rejects policy benefits from being parameter-free, and also independent of the scaling of the rewards. As a by-product of our analysis, we show that identifying the best arm (when it is unique) requires a number of samples of order (up to a log(K) factor) $\Sigma$ i 1/$\Delta$2i, where the sum is on the suboptimal arms and$\Delta$i represents the difference between the mean reward of the best arm and the one of arm i. This generalizes the well-known fact that one needs of order of 1/$\Delta$2 samples to differentiate the means of two distributions with gap $\Delta$.},
author = {Audibert, Jean-Yves and Bubeck, S{\'{e}}bastien},
booktitle = {Proceedings of the 23rd Conference on Learning Theory (CoLT)},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Best Arm Identification/Best Arm Identification in Multi-Armed Bandits.pdf:pdf},
isbn = {9780982252925},
title = {{Best arm identification in multi-armed bandits}},
url = {https://hal-enpc.archives-ouvertes.fr/hal-00654404/document},
year = {2010}
}
@article{shahriari2016loop,
abstract = {—Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
doi = {10.1109/JPROC.2015.2494218},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning},
number = {1},
pages = {148--175},
pmid = {25246403},
title = {{Taking the human out of the loop: A review of Bayesian optimization}},
url = {https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf},
volume = {104},
year = {2016}
}
@inproceedings{kaufmann2013kl,
abstract = {We consider the problem of efficiently exploring the arms of a stochastic bandit to identify the best subset of a specified size. Under the PAC and the fixed-budget formulations, we derive improved bounds by using KL-divergence-based confidence intervals. Whereas the application of a similar idea in the regret setting has yielded bounds in terms of the KL-divergence between the arms, our bounds in the pure-exploration setting involve the " Chernoff information " between the arms. In addition to introducing this novel quantity to the bandits literature, we contribute a comparison between strategies based on uniform and adaptive sampling for pure-exploration problems, finding evidence in favor of the latter.},
author = {Kaufmann, Emilie and Kalyanakrishnan, Shivaram},
booktitle = {Proceeding of the 26th Conference on Learning Theory (CoLT)},
issn = {15337928},
keywords = {kl-divergence,stochastic multi-armed bandits,subset selection},
pages = {228--251},
title = {{Information complexity in bandit subset selection}},
url = {http://proceedings.mlr.press/v30/Kaufmann13.pdf},
year = {2013}
}
@article{chan2018cbt,
abstract = {The infinite arms bandit problem was initiated by Berry et al. (1997). They derived a regret lower bound of all solutions for Bernoulli rewards, and proposed various bandit strategies based on success runs, but which do not achieve this bound. We propose here a confidence bound target (CBT) algorithm that achieves extensions of their regret lower bound for general reward distributions and distribution priors. The algorithm does not require information on the reward distributions, for each arm we require only the mean and standard deviation of its rewards to compute a confidence bound. We play the arm with the smallest confidence bound provided it is smaller than a target mean. If the confidence bounds are all larger, then we play a new arm. We show how the target mean can be computed from the prior so that the smallest asymptotic regret, among all infinite arms bandit algorithms, is achieved. We also show that in the absence of information on the prior, the target mean can be determined empirically, and that the regret achieved is comparable to the smallest regret. Numerical studies show that CBT is versatile and outperforms its competitors.},
archivePrefix = {arXiv},
arxivId = {1805.11793},
author = {Chan, Hock Peng and Hu, Shouri},
eprint = {1805.11793},
file = {:home/xuedong/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan, Hu - 2018 - Infinite arms bandit Optimality via confidence bounds.pdf:pdf},
title = {{Infinite arms bandit: Optimality via confidence bounds}},
url = {http://arxiv.org/pdf/1805.11793.pdf},
year = {2018}
}
@article{bergstra2012random,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {Bergstra, James and Bengio, Yoshua},
doi = {10.1162/153244303322533223},
eprint = {1504.05070},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
pmid = {18244602},
title = {{Random search for hyper-parameter optimization}},
url = {http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf},
volume = {13},
year = {2012}
}
@book{floudas2000,
author = {Floudas, Christodoulos A. and Pardalos, Panos M.},
booktitle = {Nonconvex optimization and its applications},
isbn = {0792361555 (alk. paper)},
keywords = {Chemistry Mathematical models.,Molecular biology Mathematical models.},
number = {40},
pages = {vii, 339 p.},
pmid = {11863955},
title = {{Optimization in computational chemistry and molecular biology : local and global approaches}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F978-1-4757-3218-4.pdf},
year = {2000}
}
@techreport{russo2017,
abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
archivePrefix = {arXiv},
arxivId = {1707.02038},
author = {Russo, Daniel and {Van Roy}, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
doi = {10.1561/XXXXXXXXX.Daniel},
eprint = {1707.02038},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Bayesian Optimization/A Tutorial on Thompson Sampling.pdf:pdf},
pages = {1--42},
title = {{A tutorial on Thompson Sampling}},
url = {https://arxiv.org/pdf/1707.02038.pdf},
volume = {1},
year = {2017}
}
@book{munos2014,
abstract = {This work covers several aspects of the optimism in the face of uncertainty principle applied to large scale optimization problems under finite numerical budget. The initial motivation for the research reported here originated from the empirical success of the so-called Monte-Carlo Tree Search method popularized in computer-go and further extended to many other games as well as optimization and planning problems. Our objective is to contribute to the development of theoretical foundations of the field by characterizing the complexity of the underlying optimization problems and designing efficient algorithms with performance guarantees.$\backslash$nThe main idea presented here is that it is possible to decompose a complex decision making problem (such as an optimization problem in a large search space) into a sequence of elementary decisions, where each decision of the sequence is solved using a (stochastic) multi-armed bandit (simple mathematical model for decision making in stochastic environments). This so-called hierarchical bandit approach (where the reward observed by a bandit in the hierarchy is itself the return of another bandit at a deeper level) possesses the nice feature of starting the exploration by a quasi-uniform sampling of the space and then focusing progressively on the most promising area, at different scales, according to the evaluations observed so far, and eventually performing a local search around the global optima of the function. The performance of the method is assessed in terms of the optimality of the returned solution as a function of the number of function evaluations.$\backslash$nOur main contribution to the field of function optimization is a class of hierarchical optimistic algorithms designed for general search spaces$\backslash$n(such as metric spaces, trees, graphs, Euclidean spaces, ...) with different algorithmic instantiations depending on whether the evaluations are noisy or noiseless and whether some measure of the “smoothness” of the function is known or unknown. The performance of the algorithms depend on the local behavior of the function around its global optima expressed in terms of the quantity of near-optimal states measured with some metric. If this local smoothness of the function is known then one can design very efficient optimization algorithms (with convergence rate independent of the space dimension), and when it is not known, we can build adaptive techniques that can, in some cases, perform almost as well as when it is known.$\backslash$nIn order to be self-contained, we start with a brief introduction to the stochastic multi-armed bandit problem in Chapter 1 and describe the UCB (Upper Confidence Bound) strategy and several extensions. In Chapter 2 we present the Monte-Carlo Tree Search method applied to computer-go and show the limitations of previous algorithms such as UCT (UCB applied to Trees). This provides motivation for designing theoretically well-founded optimistic optimization algorithms. The main contributions on hierarchical optimistic optimization are described in Chapters 3 and 4 where the general setting of a semi-metric space is introduced and algorithms designed for optimizing a function assumed to be locally smooth (around its maxima) with respect to a$\backslash$nsemi-metric are presented and analyzed. Chapter 3 considers the case when the semi-metric is known and can be used by the algorithm,$\backslash$nwhereas Chapter 4 considers the case when it is not known and describes an adaptive technique that does almost as well as when it is known. Finally in Chapter 5 we describe optimistic strategies for a specific structured problem, namely the planning problem in Markov decision processes with infinite horizon and discounted rewards setting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.0952v2},
author = {Munos, R{\'{e}}mi},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/2200000038},
eprint = {arXiv:1408.0952v2},
isbn = {9781601987662},
issn = {1935-8237},
number = {1},
pages = {1--129},
title = {{From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning}},
url = {https://hal.archives-ouvertes.fr/hal-00747575v4/document},
volume = {7},
year = {2014}
}
@inproceedings{snoek2015,
author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa A. and Prabhat and Adams, Ryan P.},
booktitle = {Proceedings of the 32nd International conference on Machine Learning (ICML)},
title = {{Scalable Bayesian optimization using deep neural networks}},
url = {https://arxiv.org/pdf/1502.05700.pdf},
year = {2015}
}
@article{berry1997infinite,
abstract = {We consider a bandit problem consisting of a sequence of {\$}n{\$} choices from an infinite number of Bernoulli arms, with {\$}n /rightarrow /infty{\$}. The objective is to minimize the long-run failure rate. The Bernoulli parameters are independent observations from a distribution {\$}F{\$}. We first assume {\$}F{\$} to be the uniform distribution on (0, 1) and consider various extensions. In the uniform case we show that the best lower bound for the expected failure proportion is between {\$}/sqrt2//sqrtn{\$} and {\$}2//sqrtn{\$} and we exhibit classes of strategies that achieve the latter.},
author = {Berry, Donald A. and Chen, Robert W. and Zame, Alan and Heath, David C. and Shepp, Larry A.},
doi = {10.1214/aos/1069362389},
isbn = {00905364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bandit problems,Dynamic allocation of bernoulli processes,Sequential experimentation,Staying with a winner,Switching with a loser},
number = {5},
pages = {2103--2116},
title = {{Bandit problems with infinitely many arms}},
url = {https://faculty.wharton.upenn.edu/wp-content/uploads/2012/04/Bandit-problems-with-infinitely-many-arms.pdf},
volume = {25},
year = {1997}
}
@article{kleinberg2008multi,
author = {Kleinberg, Robert and Slivkins, Aleksandrs and Upfal, Eli},
journal = {Symposium on Theory of Computing},
title = {{Multi-armed bandit problems in metric spaces}},
url = {https://arxiv.org/pdf/0809.4882.pdf},
year = {2008}
}
@inproceedings{qin2017ttei,
abstract = {The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.},
archivePrefix = {arXiv},
arxivId = {1705.10033},
author = {Qin, Chao and Klabjan, Diego and Russo, Daniel},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
eprint = {1705.10033},
issn = {10495258},
pages = {5381--5391},
title = {{Improving the expected improvement algorithm}},
url = {http://arxiv.org/abs/1705.10033},
year = {2017}
}
@inproceedings{hoffman2014bayesgap,
abstract = {We address the problem of finding the maximizer of a$\backslash$nnonlinear function that can only be evaluated,$\backslash$nsubject to noise, at a finite number of query$\backslash$nlocations. Further, we will assume that there is a$\backslash$nconstraint on the total number of permitted function$\backslash$nevaluations. We introduce a Bayesian approach for$\backslash$nthis problem and show that it empirically$\backslash$noutperforms both the existing frequentist$\backslash$ncounterpart and other Bayesian optimization methods.$\backslash$nThe Bayesian approach places emphasis on detailed$\backslash$nmodelling, including the modelling of correlations$\backslash$namong the arms. As a result, it can perform well in$\backslash$nsituations where the number of arms is much larger$\backslash$nthan the number of allowed function evaluation,$\backslash$nwhereas the frequentist counterpart is inapplicable.$\backslash$nThis feature enables us to develop and deploy$\backslash$npractical applications, such as automatic machine$\backslash$nlearning toolboxes. The paper presents$\backslash$ncomprehensive comparisons of the proposed approach$\backslash$nwith many Bayesian and bandit optimization$\backslash$ntechniques, the first comparison of many of these$\backslash$nmethods in the literature.},
author = {Hoffman, Matthew W. and Shahriari, Bobak and de Freitas, Nando},
booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AIStats)},
issn = {15337928},
pages = {365--374},
title = {{On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning}},
url = {http://proceedings.mlr.press/v33/hoffman14.pdf},
year = {2014}
}
@inproceedings{bubeck2009pure,
abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of forecasters that perform an on-line exploration of the arms. These forecasters are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. One of the main results in the case of a finite number of arms is a general lower bound on the simple regret of a forecaster in terms of its cumulative regret: the smaller the latter, the larger the former. Keeping this result in mind, we then exhibit upper bounds on the simple regret of some forecasters. The paper ends with a study devoted to continuous-armed bandit problems; we show that the simple regret can be minimized with respect to a family of probability distributions if and only if the cumulative regret can be minimized for it. Based on this equivalence, we are able to prove that the separable metric spaces are exactly the metric spaces on which these regrets can be minimized with respect to the family of all probability distributions with continuous mean-payoff functions.},
archivePrefix = {arXiv},
arxivId = {0802.2655},
author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi and Stoltz, Gilles},
booktitle = {Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT)},
doi = {10.1007/978-3-642-04414-4_7},
eprint = {0802.2655},
isbn = {3642044131},
issn = {03029743},
pages = {23--37},
title = {{Pure exploration in multi-armed bandits problems}},
url = {https://arxiv.org/pdf/0802.2655.pdf},
year = {2009}
}
@book{schrijver2003,
abstract = {This book offers an in-depth overview of polyhedral methods and efficient algorithms in combinatorial optimization.These methods form a broad, coherent and powerful kernel in combinatorial optimization, with strong links to discrete mathematics, mathematical programming and computer science. In eight parts, various areas are treated, each starting with an elementary introduction to the area, with short, elegant proofs of the principal results, and each evolving to the more advanced methods and results, with full proofs of some of the deepest theorems in the area. Over 4000 references to further research are given, and historical surveys on the basic subjects are presented.},
author = {Schrijver, Alexander},
doi = {10.1007/s10288-004-0035-9},
isbn = {ISBN 978-3-540-44389-6},
issn = {1619-4500},
pmid = {12975452},
publisher = {Springer Science {\&} Business Media},
title = {{Combinatorial Optimization: Polyhedra and Efficiency}},
url = {https://homepages.cwi.nl/{~}lex/files/book.pdf},
volume = {24},
year = {2003}
}
@inproceedings{srinivas2010gpucb,
abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
archivePrefix = {arXiv},
arxivId = {0912.3995},
author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
booktitle = {Proceedings of the 27th International conference on Machine Learning (ICML)},
doi = {10.1109/TIT.2011.2182033},
eprint = {0912.3995},
isbn = {9781605589077},
issn = {00189448},
pages = {1015--1022},
title = {{Gaussian process optimization in the bandit setting: No regret and experimental design}},
url = {https://arxiv.org/pdf/0912.3995.pdf},
year = {2010}
}
@inproceedings{kalyanakrishnan2012lucb,
author = {Kalyanakrishnan, Shivaram and Tewari, Ambuj and Auer, Peter and Stone, Peter},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
isbn = {978-1-4503-1285-1},
keywords = {stochastic multi-armed bandits,subset selection},
pages = {655--662},
title = {{PAC subset selection in stochastic multi-armed bandits}},
url = {https://icml.cc/2012/papers/359.pdf},
year = {2012}
}
@inproceedings{russo2016ttts,
abstract = {This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. This paper proposes three simple and intuitive Bayesian algorithms for adaptively allocating measurement effort, and formalizes a sense in which these seemingly naive rules are the best possible. One proposal is top-two probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant of top-two sampling which considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. We prove that these simple algorithms satisfy a sharp optimality property. In a frequentist setting where the true quality of the designs is fixed, one hopes the posterior definitively identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. We show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation},
archivePrefix = {arXiv},
arxivId = {1602.08448},
author = {Russo, Daniel},
booktitle = {Proceeding of the 29th Conference on Learning Theory (CoLT)},
eprint = {1602.08448},
title = {{Simple Bayesian algorithms for best arm identification}},
url = {http://arxiv.org/pdf/1602.08448.pdf},
year = {2016}
}
@article{bubeck2010x,
abstract = {We consider a generalization of stochastic bandits where the set of arms, {\$}\backslashcX{\$}, is allowed to be a generic measurable space and the mean-payoff function is "locally Lipschitz" with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if {\$}\backslashcX{\$} is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by {\$}\backslashsqrt{\{}n{\}}{\$}, i.e., the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches.},
archivePrefix = {arXiv},
arxivId = {1001.4475},
author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi and Stoltz, Gilles and Szepesvari, Csaba},
eprint = {1001.4475},
file = {:home/xuedong/Documents/xuedong/phd/work/references/Major/Bandit Theory/Continuum-armed Bandits/X-Armed Bandits.pdf:pdf},
journal = {Journal of Machine Learning Research},
month = {jan},
pages = {1587--1627},
title = {{X-armed bandits}},
url = {http://arxiv.org/pdf/1001.4475.pdf},
volume = {12},
year = {2010}
}
@article{moles2003,
author = {Moles, Carmen G. and Mendes, Pedro and Banga, Julio R.},
doi = {10.1101/gr.1262503.},
journal = {Genome Research},
pages = {2467--2474},
title = {{Parameter estimation in biochemical pathways: A comparison of global optimization methods}},
url = {https://genome.cshlp.org/content/13/11/2467.full.pdf+html},
year = {2003}
}
@inproceedings{agarwal2014iloveconbandits,
abstract = {We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of {\$}K{\$} actions in response to the observed context, and observes the reward only for that chosen action. Our method assumes access to an oracle for solving fully supervised cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only {\$}\backslashtilde{\{}O{\}}(\backslashsqrt{\{}KT/\backslashlog N{\}}){\$} oracle calls across all {\$}T{\$} rounds, where {\$}N{\$} is the number of policies in the policy class we compete against. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines.},
archivePrefix = {arXiv},
arxivId = {1402.0555},
author = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert E.},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML)},
eprint = {1402.0555},
isbn = {9781634393973},
issn = {10769757},
pages = {1638--1646},
pmid = {17255001},
title = {{Taming the monster: A fast and simple algorithm for contextual bandits}},
url = {http://arxiv.org/pdf/1402.0555.pdf},
year = {2014}
}
@inproceedings{auer2007improved,
abstract = {Considering one-dimensional continuum-armed bandit problems, we propose an improvement of an algorithm of Kleinberg and a new set of conditions which give rise to improved rates. In particular, we introduce a novel assumption that is complementary to the previous smoothness conditions, while at the same time smoothness of the mean payoff function is required only at the maxima. Under these new assumptions new bounds on the expected regret are derived. In particular, we show that apart from logarithmic factors, the expected regret scales with the square-root of the number of trials, provided that the mean payoff function has finitely many maxima and its second derivatives are continuous and non-vanishing at the maxima. This improves a previous result of Cope by weakening the assumptions on the function. We also derive matching lower bounds. To complement the bounds on the expected regret, we provide high probability bounds which exhibit similar scaling.},
author = {Auer, Peter and Ortner, Ronald and Szepesv{\'{a}}ri, Csaba},
booktitle = {Proceedings of the 21th Conference on Learning Theory (CoLT)},
doi = {10.1007/978-3-540-72927-3_33},
isbn = {978-3-540-72925-9},
issn = {03029743},
pages = {454--468},
title = {{Improved rates for the stochastic continuum-armed bandit problem}},
url = {http://dx.doi.org/10.1007/978-3-540-72927-3{\_}33{\%}5Cnhttp://www.springerlink.com/index/440117663L6X5X77.pdf},
volume = {4539},
year = {2007}
}
@inproceedings{samothrakis2013,
author = {Samothrakis, Spyridon and Perez, Diego and Lucas, Simon},
booktitle = {NIPS Workshop on Causality},
title = {{Training gradient boosting machines using curve-fitting and information-theoretic features for causal direction detection}},
url = {http://ssamot.me/papers/Samothrakis-NIPS2013-causality.pdf},
year = {2013}
}
@article{kandasamy2017,
abstract = {We design and analyse variations of the classical Thompson sampling (TS) procedure for Bayesian optimisation (BO) in settings where function evaluations are expensive, but can be performed in parallel. Our theoretical analysis shows that a direct application of the sequential Thompson sampling algorithm in either synchronous or asynchronous parallel settings yields a surprisingly powerful result: making {\$}n{\$} evaluations distributed among {\$}M{\$} workers is essentially equivalent to performing {\$}n{\$} evaluations in sequence. Further, by modeling the time taken to complete a function evaluation, we show that, under a time constraint, asynchronously parallel TS achieves asymptotically lower regret than both the synchronous and sequential versions. These results are complemented by an experimental analysis, showing that asynchronous TS outperforms a suite of existing parallel BO algorithms in simulations and in a hyper-parameter tuning application in convolutional neural networks. In addition to these, the proposed procedure is conceptually and computationally much simpler than existing work for parallel BO.},
archivePrefix = {arXiv},
arxivId = {1705.09236},
author = {Kandasamy, Kirthevasan and Krishnamurthy, Akshay and Schneider, Jeff and Poczos, Barnabas},
eprint = {1705.09236},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Bayesian Optimization/Asynchronous Parallel Bayesian Optimisation via Thompson Sampling.pdf:pdf},
title = {{Asynchronous parallel Bayesian optimisation via Thompson Sampling}},
url = {http://arxiv.org/pdf/1705.09236.pdf},
year = {2017}
}
@inproceedings{franceschi2018bilevel,
abstract = {We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning. We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective. Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem. We instantiate our approach for meta-learning in the case of deep learning where representation layers are treated as hyperparameters shared across a set of training episodes. In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.},
archivePrefix = {arXiv},
arxivId = {1806.04910},
author = {Franceschi, Luca and Frasconi, Paolo and Salzo, Saverio and Pontil, Massimilano},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1806.04910},
issn = {1938-7228},
title = {{Bilevel programming for hyperparameter optimization and meta-learning}},
url = {http://arxiv.org/pdf/1806.04910.pdf},
year = {2018}
}
@article{agrawal1995continuum,
abstract = {In this paper we consider the multiarmed bandit problem where the arms are chosen from a subset of the real line and the mean rewards are assumed to be a continuous function of the arms. The problem with an infinite number of arms is much more difficult than the usual one with a finite number of arms because the built-in learning task is now infinite dimensional. We devise a kernel estimator-based learning scheme for the mean reward as a function of the arms. Using this learning scheme, we construct a class of certainty equivalence control with forcing schemes and derive asymptotic upper bounds on their learning loss. To the best of our knowledge, these bounds are the strongest rates yet available. Moreover, they are stronger than the o(n) required for optimality with respect to the average-cost-per-unit-time criterion.},
author = {Agrawal, Rajeev},
doi = {10.1137/S0363012992237273},
issn = {0363-0129},
journal = {SIAM Journal on Control and Optimization},
keywords = {bandit problems,certainty,continuous arms,controlled i.i.d,equivalence with forcing,learning loss,process,stochastic adaptive control},
number = {6},
pages = {1926--1951},
title = {{The continuum-armed bandit problem}},
url = {https://epubs.siam.org/doi/pdf/10.1137/S0363012992237273},
volume = {33},
year = {1995}
}
@article{lecun1998gradient,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
url = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
volume = {86},
year = {1998}
}
@inproceedings{garivier2016tracknstop,
abstract = {We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the `Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis.},
archivePrefix = {arXiv},
arxivId = {1602.04589},
author = {Garivier, Aur{\'{e}}lien and Kaufmann, Emilie},
booktitle = {Proceedings of the 29th Conference on Learning Theory (CoLT)},
eprint = {1602.04589},
title = {{Optimal best arm identification with fixed confidence}},
url = {http://arxiv.org/pdf/1602.04589.pdf},
year = {2016}
}
@inproceedings{xia2015budgeted,
abstract = {Thompson sampling is one of the earliest randomized algorithms for multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to Budgeted MAB, where there is random cost for pulling an arm and the total cost is constrained by a budget. We start with the case of Bernoulli bandits, in which the random rewards (costs) of an arm are independently sampled from a Bernoulli distribution. To implement the Thompson sampling algorithm in this case, at each round, we sample two numbers from the posterior distributions of the reward and cost for each arm, obtain their ratio, select the arm with the maximum ratio, and then update the posterior distributions. We prove that the distribution-dependent regret bound of this algorithm is {\$}O(\backslashln B){\$}, where {\$}B{\$} denotes the budget. By introducing a Bernoulli trial, we further extend this algorithm to the setting that the rewards (costs) are drawn from general distributions, and prove that its regret bound remains almost the same. Our simulation results demonstrate the effectiveness of the proposed algorithm.},
archivePrefix = {arXiv},
arxivId = {1505.00146},
author = {Xia, Yingce and Li, Haifang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
booktitle = {Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI)},
eprint = {1505.00146},
isbn = {9781577357384},
issn = {10450823},
month = {may},
pages = {3960--3966},
title = {{Thompson Sampling for budgeted multi-armed Bandits}},
url = {http://arxiv.org/pdf/1505.00146.pdf},
year = {2015}
}
@article{pedregosa2011sklearn,
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine learning in Python}},
url = {http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf},
volume = {12},
year = {2011}
}
@article{srivastava2014dropout,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/JMLRdropout.pdf},
volume = {15},
year = {2014}
}
@inproceedings{snoek2012spearmint,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
booktitle = {Advances in Neural Information Processing Systems 25 (NIPS)},
isbn = {9781627480031},
pages = {2951--2959},
title = {{Practical bayesian optimization of machine learning algorithms}},
url = {https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
year = {2012}
}
@inproceedings{grill2015poo,
abstract = {We study the problem of black-box optimization of a function f of any dimension, given function evaluations perturbed by noise. The function is assumed to be locally smooth around one of its global optima, but this smoothness is unknown. Our contribution is an adaptive optimization algorithm, POO or parallel optimistic optimization, that is able to deal with this setting. POO performs almost as well as the best known algorithms requiring the knowledge of the smoothness. Furthermore, POO works for a larger class of functions than what was previously considered, especially for functions that are difficult to optimize, in a very precise sense. We provide a finite-time analysis of POO's performance, which shows that its error after n evaluations is at most a factor of sqrt(ln n) away from the error of the best known optimization algorithms using the knowledge of the smoothness.},
author = {Grill, Jean-Bastien and Valko, Michal and Munos, R{\'{e}}mi},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS)},
file = {:home/xuedong/Documents/xuedong/phd/work/references/Major/Bandit Theory/Continuum-armed Bandits/Black-box Optimization of Noisy Functions with Unknown Smoothness.pdf:pdf},
issn = {10495258},
pages = {667--675},
title = {{Black-box optimization of noisy functions with unknown smoothness}},
url = {http://researchers.lille.inria.fr/{~}valko/hp/publications/grill2015black-box.pdf},
year = {2015}
}
@book{sutton1998,
author = {Sutton, Richard S. and Barto, Andrew G.},
publisher = {MIT Press},
title = {{Reinforcement Learning: An Introduction}},
url = {http://www.incompleteideas.net/book/bookdraft2018mar21.pdf},
year = {1998}
}
@article{robbins1952,
author = {Robbins, Herbert},
doi = {10.1090/S0002-9904-1952-09620-8},
journal = {Bulletin of the American Mathematics Society},
number = {5},
pages = {527--535},
title = {{Some aspects of the sequential design of experiments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3232{\&}rep=rep1{\&}type=pdf},
volume = {58},
year = {1952}
}
@inproceedings{carpentier2015siri,
abstract = {We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter {\$}\backslashbeta{\$} characterizing the distribution of the near-optimal arms. We prove that depending on {\$}\backslashbeta{\$}, our algorithm is minimax optimal either up to a multiplicative constant or up to a {\$}\backslashlog(n){\$} factor. We also provide extensions to several important cases: when {\$}\backslashbeta{\$} is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.},
archivePrefix = {arXiv},
arxivId = {1505.04627},
author = {Carpentier, Alexandra and Valko, Michal},
booktitle = {Proceedings of the 32nd International conference on Machine Learning (ICML)},
eprint = {1505.04627},
isbn = {9781510810587},
pages = {1133--1141},
title = {{Simple regret for infinitely many armed bandits}},
url = {http://arxiv.org/pdf/1505.04627.pdf},
year = {2015}
}
@article{mockus1978,
abstract = {The purpose of this paper is to describe how the Bayesian approach can be applied ot the global optimization of multiextremal functions. The function to be minimized is considered as a realizatin of some stochastic function. The optimization technique based upon the minimization of the expected deviation from the extremum is called Bayesian. The implementation of Bayesian methods is considered. The results of the application to the minimization of some standard test functions are given.},
author = {Mockus, Jonas and Tie{\v{s}}is, Vytautas and {\v{Z}}ilinskas, Antanas},
doi = {10.1007/978-94-009-0909-0_8},
isbn = {9781479932795},
journal = {Towards Global Optimisation 2},
pages = {117--129},
title = {{The application of Bayesian methods for seeking the extremum}},
url = {https://www.researchgate.net/publication/248818761{\_}The{\_}application{\_}of{\_}Bayesian{\_}methods{\_}for{\_}seeking{\_}the{\_}extremum},
year = {1978}
}
@inproceedings{agrawal2011analysis,
abstract = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the multi-armed bandit problem. More precisely, for the two-armed bandit problem, the expected regret in time {\$}T{\$} is {\$}O(\backslashfrac{\{}\backslashln T{\}}{\{}\backslashDelta{\}} + \backslashfrac{\{}1{\}}{\{}\backslashDelta{\^{}}3{\}}){\$}. And, for the {\$}N{\$}-armed bandit problem, the expected regret in time {\$}T{\$} is {\$}O([(\backslashsum{\_}{\{}i=2{\}}{\^{}}N \backslashfrac{\{}1{\}}{\{}\backslashDelta{\_}i{\^{}}2{\}}){\^{}}2] \backslashln T){\$}. Our bounds are optimal but for the dependence on {\$}\backslashDelta{\_}i{\$} and the constant factors in big-Oh.},
archivePrefix = {arXiv},
arxivId = {1111.1797},
author = {Agrawal, Shipra and Goyal, Navin},
booktitle = {Proceedings of the 25th Conference on Learning Theory (CoLT)},
doi = {arXiv:1111.1797},
eprint = {1111.1797},
file = {:home/xuedong/Dropbox/xuedong2017/phd/reading/Bayesian Optimization/Analysis of Thompson Sampling for the Multi-armed Bandit Problem.pdf:pdf},
isbn = {0960-3115},
issn = {15337928},
pages = {1--26},
pmid = {618},
title = {{Analysis of Thompson Sampling for the multi-armed bandit problem}},
url = {http://arxiv.org/pdf/1111.1797.pdf},
year = {2011}
}
